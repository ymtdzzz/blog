#+HUGO_BASE_DIR: ./
#+HUGO_SECTION: posts
#+author: zeroclock

* Blog entries
  :PROPERTIES:
  :VISIBILITY: children
  :END:
** DONE Goのhot reloadにgo-taskを使ってみる                  :go:task:docker:
   :PROPERTIES:
   :EXPORT_FILE_NAME: using-ga-task-to-host-reload
   :EXPORT_HUGO_SECTION: /posts/2020/07
   :EXPORT_DATE: 2020-07-08
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :toc true
   :END:
*** Goでhot reloading
    作っているアプリのサーバサイドをGOで書いているので、[[https://github.com/oxequa/realize][Realize]]でhot reloadを実現しようと思ったのですが、 ~GO111MODULE=off~ にしないとgo getできなかったり、いざdocker-composeで ~realize start --run~ しようとすると下記のようなエラーが出たりと色々あれだったので、他に使えそうなパッケージが無いか探してみました。

    #+CAPTION: docker-composeでrealize startした際のエラー
    #+BEGIN_SRC
...
[01:09:01][SRC] : Running..
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x4cf2fb]

goroutine 8768 [running]:
os.(*Process).signal(0x0, 0xad7a20, 0xe34878, 0x0, 0x0)
	/usr/local/go/src/os/exec_unix.go:56 +0x3b
os.(*Process).Signal(...)
	/usr/local/go/src/os/exec.go:131
github.com/oxequa/realize/realize.(*Project).run.func1(0xc000175698)
	/go/src/github.com/oxequa/realize/realize/projects.go:581 +0x5c
github.com/oxequa/realize/realize.(*Project).run(0xc0001fa000, 0xc000133ab8, 0x7, 0xc000342300, 0xc000110540, 0xad2c20, 0xc00011c8d0)
	/go/src/github.com/oxequa/realize/realize/projects.go:646 +0xc2d
github.com/oxequa/realize/realize.(*Project).Reload.func3(0xc0001fa000, 0xc000342300, 0xc000110540)
	/go/src/github.com/oxequa/realize/realize/projects.go:262 +0x147
created by github.com/oxequa/realize/realize.(*Project).Reload
	/go/src/github.com/oxequa/realize/realize/projects.go:260 +0x297
    #+END_SRC

    調べたところ、[[https://github.com/go-task/task][go-task]] が中々シンプルで良さそうだったので試してみました。

*** go-taskの使い方
    基本的な使い方は下記の通り。

**** go-taskのインストール
     [[https://taskfile.dev/#/installation][ドキュメント]] にあるように、MacとLinux(Linuxbrew導入済)は =brew= で、Windowsの場合は =scoop= とかでサクッとインストールできるみたいです。

     ただ、私の場合はdocker上のdebianでインストールしたかったので +dockerでlinuxbrew入れるの地味にめんどうなので+ バイナリを =dpkg= でインストールしました。

     #+caption: Debianにおいて、バイナリ(.deb)をdpkgでインストールするコマンド
     #+BEGIN_SRC bash
wget https://github.com/go-task/task/releases/download/v2.8.1/task_linux_amd64.deb
dpkg -i task_linux_amd64.deb
rm task_linux_amd64.deb
     #+END_SRC

**** Taskfile.ymlの作成
     go-taskでは諸々の設定をTaskfile.ymlに記述しますので、プロジェクトルートにTaskfile.ymlを作成します。 =task init= でサクッと作ってくれます。

     #+BEGIN_SRC bash
cd /path/to/project/root
task init
     #+END_SRC

     ソースをウォッチして特定のコマンド（go runとか）を実行させたい場合、こんな感じに書けます

     #+BEGIN_SRC yaml
version: '2'

tasks:
  run:
    cmds:
      - go run main.go
    sources:
      - ./**/*
     #+END_SRC

**** 実行
     あとはTaskfile.ymlがあるディレクトリで実行するだけです。

     #+BEGIN_SRC bash
# runはTaskfile.ymlで指定したタスク名
task run
     #+END_SRC
*** 自分の使い方
    プレイベートで開発しているアプリが、Goで書いたローカルサーバでReactを配信する　といった構成になってます。Goのサーバはリソース配信用とAPI兼用になっており、開発中はDocker container上で動かします。シンプルな構成なのでついでに記載しておきます。

    フォルダ構成（抜粋）は下記の通り。

    #+BEGIN_SRC plantuml :file overview.png :cache yes :cmdline -config "$HOME/.emacs.d/styles.uml" :async
@startsalt
{
{T
+src
++web
+++src
+++public
+++node_modules
+++build
+++package.json
++main.go
++handler
+++router.go
+++middleware.go
++config
++startup.sh
++Taskfile.yml
++Dockerfile
++docker-compose.yml
}
}
@endsalt
    #+END_SRC

    #+caption: プロジェクトのフォルダ構成（一部抜粋）
    #+RESULTS[47167a60cfd3c4776dd5c164046eb37355045db1]:
    [[file:overview.png]]

**** Dockerfile
     npmも入れています。

     #+BEGIN_SRC Dockerfile
FROM golang:1.14.4

WORKDIR /go/src

ENV GO111MODULE=on

pCOPY . /go/src

RUN apt-get update \
    && apt-get install -y git python jq curl \
    && curl -sL https://deb.nodesource.com/setup_14.x | bash - \
    && apt-get update && apt-get install -y nodejs \
    && npm install yarn -g \
    && wget https://github.com/go-task/task/releases/download/v2.8.1/task_linux_amd64.deb \
    && dpkg -i task_linux_amd64.deb \
    && rm task_linux_amd64.deb

EXPOSE 8080

CMD ["task", "run"]
     #+END_SRC

**** Taskfile
     #+BEGIN_SRC yaml
version: '2'

tasks:
  run:
    cmds:
      - cmd: kill -TERM `cat pidfile`
        ignore_error: true
      - go run main.go --pid-file=pidfile
    sources:
      - ./**/*
     #+END_SRC

     =go run main.go= だけだとフォルダ変更を検知する度に前に走っていたプロセスを落とさずにまた別プロセスとして起動してしまうので、pidを適当にどこかに吐き出しておいて、起動時は前のプロセスをkillしてから実行するようにしています（[[https://qiita.com/croquette0212/items/dab91c1075c1f3ac7b8d][go-taskでサーバーのライブリロードを実現する]] を参考にさせていただきました）。

     これでファイル変更を検知してホットリロードしてくれます。

**** 所感
     環境構築というプロジェクトの本質に関わらない部分については、なるべくエネルギーを割きたくないのですが、go-taskのおかげで自分が作りたいものに集中できています。

     実行済タスクのkillの仕方は若干ゴリっぽい側面があるので、もうちょいスマートにいけないか考え中です。ただ、Taskfile作ってコマンド叩くだけでいいというシンプルなワークフローは気に入ったので、しばらく使ってみたいと思います。
** DONE 【Typescript】axiosのレスポンスはきちんと型チェックしよう :typescript:axios:
   :PROPERTIES:
   :EXPORT_FILE_NAME: type-checking-the-response-via-axios
   :EXPORT_HUGO_SECTION: /posts/2020/08
   :EXPORT_DATE: 2020-08-12
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
*** Axiosでエラー
    Axiosで外部APIを叩いてデータを取得したいと思い、下記のコードを書いたとします。

    #+caption: AxiosでAPIを叩いて情報を取得するコード例
    #+BEGIN_SRC typescript
import axios, { AxiosPromise } from "axios";

interface CatApiResponse {
  name: string;
  age: number;
  parents: string[];
}

const client = axios.create({
  baseURL: "https://example.com/api/v2/",
  headers: {
    "Content-Type": "application/json"
  }
});

const fetchAllCat = (): AxiosPromise<CatApiResponse> => client.get("cat");

const hoge = () => {
  const data = fetchAllCat();
  data.then((data) => {
    data.data.parents.map((parent) => {
      console.log(parent);
      return "hoge";
    });
  });
};
    #+END_SRC

    IDEで型推定を確認すると、確かに ~CatApiResponse~ になっている。

    #+DOWNLOADED: clipboard @ 2020-08-12 17:55:51
    #+CAPTION: レスポンスデータの型推定
    [[file:blog.org_imgs/20200812_175551.png]]

    けど、実際はnullかもしれないし、 ~CatApiResponse~ のinterfaceに則したデータ構造じゃないかもしれない。で、実際に変なデータを返すAPIを用意して実行すると、案の定 ~data.data.parents.map()~ のところでコケる。でも、IDEにも怒られないし、コンパイル時にもツッコまれない。

*** カスタム型ガードでちゃんとチェックする
    イマイチ釈然としないけど、型ガードでちゃんとデータをチェックしてから返却しよう　というお話。

    #+caption: CatApiResponseの型ガード例
    #+BEGIN_SRC typescript
const isCatApiResnpose = (arg: any): arg is CatApiResponse => {
  return (arg.name !== undefined
    && arg.age !== undefined
    && arg.parents !== undefined
    && Array.isArray(arg.parents))
}
    #+END_SRC

    こんな感じの型ガードを書いてあげて、 ~fetchAllCat()~ で受け取ったPromiseをresolveしたときに、きちんとデータがCapApiResponseのinterfaceに準拠していることを確認してあげる必要がある。

    #+caption: きちんと型チェックを行う例
    #+BEGIN_SRC typescript
const hoge = () => {
  const data = fetchAllCat();
  data.then((data) => {
    if (isCatApiResnpose(data.data)) {
      data.data.parents.map((parent) => {
        console.log(parent);
        return "hoge";
      });
    }
  });
};
    #+END_SRC

    こうすることで、はちゃめちゃなデータが返ってきても安全に処理ができる（これでいいのか...?）。

    実際はReactでデータをstateにsetしたりすることもあるが、その際はnullとか想定外のデータ構造だった場合は空のCatApiResponseを準備して返して上げれば単なる「データ無し」として扱える。

    で、ここで面倒なのが、「空のhoge interfaceのデータ」を作ることで、構造が複雑だと一々手動でemptyHogeDataみたいなものを作らないといけない。ただ、その場合は該当するinterfaceを実装したclassを作っちゃって、そのconstructorで空を作らせるのも手かな　と。


    ということで、今回はtypescriptのお話でございました。
    
** TODO TauriでWebの技術でネイティブアプリを作る                      :tauri:
   :PROPERTIES:
   :EXPORT_FILE_NAME: building-native-app-with-tauri
   :EXPORT_HUGO_SECTION: /posts/2020/07
   :EXPORT_DATE: 2020-07-10
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
** DONE EmacsのLSP-modeの動作を軽くする           :Emacs:lspmode:performance:
   :PROPERTIES:
   :EXPORT_FILE_NAME: emacs-lsp-mode-more-faster
   :EXPORT_HUGO_SECTION: /posts/2020/07
   :EXPORT_DATE: 2020-07-11
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
   EmacsのLSP-modeは非常に快適で、言語サポートの追加も簡単にできるので重宝しているのですが、動作がカクついたりしてストレスになる場合がありました。[[https://emacs-lsp.github.io/lsp-mode/page/performance/][ドキュメント]]を確認したところ、パフォーマンスチューニングの方法があったのでまとめておきます。

*** いざチューニング
    今回対応するチューニングが正常に適用されているかどうかは、 =M-x lsp-diagnose= で確認できます。

    #+caption: lsp-diagnoseの出力結果
    #+BEGIN_SRC
Checking for Native JSON support: OK
Checking emacs version has `read-process-output-max': OK
Using company-capf: OK
Check emacs supports `read-process-output-max': ERROR
Check `read-process-output-max' default has been changed from 4k: ERROR
Byte compiled against Native JSON (recompile lsp-mode if failing when Native JSON available): ERROR
`gc-cons-threshold' increased?: ERROR
    #+END_SRC

    以前company-capfだけ有効化していたので、 ~Using company-capf~ がOKになっていますが、company-lspを使用している場合はERRORになるかと思います。

    また、私の環境はEmacs-plus@28でネイティブJSONパーサ（後述）入りでビルドしたものなので、 =Native JSON support= と =emacs version has `read-process-ourput-max`= がOKになっています。Emacs26とかだとERRORになるかもしれませんので、アップデートが必要です。

    それでは実際に各チューニング内容を確認していきます。

**** EmacsのネイティブJSONパーサを使う
     Emacsはver.27以降は、ネイティブでJSONのパースをサポートするようになりました。

     ただし、ver.27以降でも、コンパイル時に =--with-json= オプションが渡されていないとサポートされないみたいです。

     自分が使用しているEmacsが対応しているかどうかは =M-: (functionp 'json-serialize)= で確認できます。

     ちなみに、MacでEmacs-plusを使用する場合は、 =brew install emacs@28 --with-jansson= でインストールできます。

     Elispのパーサよりも、ネイティブパーサの方が最大15倍程度まで高速化されるらしい(Benchmarks show that Emacs 27 is ~15 times faster than Emacs when using Elisp json parser implementation.)ので、この設定は絶対ON推奨です。

**** gc-cons-threshold の調整
     =gc-cons-threshold= は、ガベージコレクションを実行する閾値ですが、デフォルト設定だとLSP-server/client間のデータやり取りに対して少なすぎるため、増やしてあげる必要があります。

     調製の仕方は下記の２通り紹介されていました。

     - 100mbくらいの大きな値をドカッと割り当てる（doomとかspacemacsとかも同じような設定）
     - 初期設定を２倍していき、２倍してもレスポンスに変化が見られない時点で増加をストップさせ、それを設定値とする

     後者の設定方法についてはGNU EmacsのメンテナであるEli Zaretskii氏のおすすめなので、一旦はこれに従って設定しました。

     現在の設定値を確認するためには =M-x eval-expression gc-cons-threshold= で確認できます。ちなみに私の環境だと800,000(80KB)でした。

     毎回init.elを書き換えて増やしていくのはめんどうなので、 =M-x eval-expression (seta gc-cons-threshold 1600000)= のような感じで少しずつ増やして様子を見てみたところ、丁度6,400,000と12,800,000の辺りでサジェストの出方がスムーズになり、それ以上増やしてもそこまで変化が無かったので、12,800,000(12MB)あたりにしておきました。

**** company-lsp ではなく company-capf を使用する
     今はcompany-lspは非推奨になっているので、company-capfを使用する設定を行います。

     ドキュメントだと =(setq lsp-prefer-capf t)= だけでいいとのことだったのですが、私の環境だとcompany-backendsにcompany-capfが入ってくれなかったので、 下記のように明示的に設定しています（use-packageでlsp-mode読み込んでるとこ）。

     #+caption: 明示的にcompany-capfを使用する設定
     #+begin_src elisp
  :hook
  (lsp-mode . lsp-ui-mode)
  (lsp-managed-mode . (lambda () (setq-local company-backends '(company-capf))))
     #+end_src

     設定の確認は =M-x company-diag= でできます。

**** (Windowsの場合)lsp-uiを無効化する
     Windowsだとlsp-uiが悪さをして遅くなることがあるみたいです。私はMacなのでスルー。

**** lsp-idle-delay の調整
     タイピング中にどれくらいの頻度でLSP系の状態（ハイライトとか）を更新するかの値ですが、これはとりあえず初期値の0.5のままにしました。

*** 最終確認
    以上のチューニング完了後、再び =lsp-diagnose= を実行すると、下記のような出力になるかと思います。

    #+caption: やったぜ
    #+begin_src
Checking for Native JSON support: OK
Checking emacs version has `read-process-output-max': OK
Using company-capf: OK
Check emacs supports `read-process-output-max': OK
Check `read-process-output-max' default has been changed from 4k: OK
Byte compiled against Native JSON (recompile lsp-mode if failing when Native JSON available): OK
`gc-cons-threshold' increased?: OK
    #+end_src

** DONE Lambciとimg2lambdaとserverlessでLambdaのデプロイフローを構築する :AWS:Lambda:lambci:img2lambda:PHP:CustomRuntime:serverless:
   :PROPERTIES:
   :EXPORT_FILE_NAME: deploy-lambda-with-lambci-img2lambda-serverless
   :EXPORT_HUGO_SECTION: /posts/2020/09
   :EXPORT_DATE: 2020-09-02
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
*** Lambdaのローカル環境
    これまでLambdaを構築する際には、ソースコードを決め打ちで書いてzipで上げたり、コンソール上のエディタでポチポチ開発していたりしてました。

    PythonとかNodejsとかなら、それでも簡単なAPIくらいなら作れるのですが、ちょっと複雑なことになったり、PHPみたいにCustom Runtimeを使いたい場合とかは、何度もデプロイし直してトライアンドエラーするのは効率が悪いです。

    やっぱり、他のソースと同じようにローカルでガリガリ書いて、コマンドで自動デプロイができた方が良いので色々探したところ、Lambciとimg2lambda（あとserverless）を使ったフローが良さそうだったので紹介します。

    <!--more-->
*** lambciとimg2lambda
    はじめに、各ツールの概要を軽く説明します。

**** lambci/lambda
     [[https://hub.docker.com/r/lambci/lambda/][lambci/lambda]] は、Lambdaの環境に非常に近いDockerイメージです。PythonのようなLambdaでデフォルトでサポートしている言語であれば、このイメージをPullしてファイルを配置するだけでLambdaのローカル開発環境がサクッと作れちゃいます。

     PHPの場合はCustom Runtimeを作成すれば問題無く動作します（今回の記事で解説）。

**** img2lambda
     [[https://github.com/awslabs/aws-lambda-container-image-converter][AWS Lambda Container Image Converter(略してimg2lambda)]] は、Dockerコンテナ上のソースコードをLambdaにデプロイ可能なzipファイルに固めてくれるツールです。

     配置するコードは下記のルールに従います。

#+BEGIN_QUOTE
     - /var/task : Lambdaのソースコード本体
     - /opt : Lambdaレイヤー
#+END_QUOTE

     よって、 ~/opt~ 配下にPHPを動かすためのバイナリとかライブラリ系を配置すれば、Custom Runtimeであってもきちんと固めてくれます。

**** Serverless Framework
     おなじみの [[https://www.serverless.com/][serverless framework]] ですが、これは、作成したyamlテンプレートの通りに自動デプロイしてくれるツールです。構成情報をコード化して管理する　という面ではCloudFormationと同じですが、もっと手軽に記述することができます（serverlessがCloudFormationに変換してくれる）。

     今回は、img2lambdaで固めたzipとテンプレートファイルをインプットにして、コマンド一発でデプロイするのに使用します。

*** 全体像
    流れを図式化すると、下記のような感じです。

#+DOWNLOADED: clipboard @ 2020-09-01 21:04:18
#+CAPTION: 全体の流れ
[[file:blog.org_imgs/20200901_210418.png]]

    ローカルで開発したイメージをそのままLambdaにデプロイできるので、スムーズにLambdaの開発を行うことができます。

*** 実際に作ってみる
    今回は、カスタムランタイムを使いたいのでPHPでやってみたいと思います。

**** lambciによるローカル開発環境のセットアップ
     まずは適当にディレクトリを作ってもらって、Dockerfileを作成します。といっても、基本的なところはimg2lambdaのexampleとほぼ同じです。

#+BEGIN_SRC dockerfile
#+CAPTION: Dockerfile
# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

####### PHP custom runtime #######
####### Install and compile everything #######
# Same AL version as Lambda execution environment AMI
FROM amazonlinux:2018.03.0.20190514 as builder

# Set desired PHP Version
ARG php_version="7.3.6"

# Lock to 2018.03 release (same as Lambda) and install compilation dependencies
RUN sed -i 's;^releasever.*;releasever=2018.03;;' /etc/yum.conf && \
    yum clean all && \
    yum install -y autoconf \
                bison \
                bzip2-devel \
                gcc \
                gcc-c++ \
                git \
                gzip \
                libcurl-devel \
                libxml2-devel \
                make \
                openssl-devel \
                tar \
                unzip \
                zip

# Download the PHP source, compile, and install both PHP and Composer
RUN curl -sL https://github.com/php/php-src/archive/php-${php_version}.tar.gz | tar -xvz && \
    cd php-src-php-${php_version} && \
    ./buildconf --force && \
    ./configure --prefix=/opt/php-7-bin/ --with-openssl --with-curl --with-zlib --without-pear --enable-bcmath --with-bz2 --enable-mbstring --with-mysqli && \
    make -j 5 && \
    make install && \
    /opt/php-7-bin/bin/php -v && \
    curl -sS https://getcomposer.org/installer | /opt/php-7-bin/bin/php -- --install-dir=/opt/php-7-bin/bin/ --filename=composer

# Prepare runtime files
RUN mkdir -p /lambda-php-runtime/bin && \
    cp /opt/php-7-bin/bin/php /lambda-php-runtime/bin/php

COPY runtime/bootstrap /lambda-php-runtime/
RUN chmod 0555 /lambda-php-runtime/bootstrap

RUN /opt/php-7-bin/bin/php /opt/php-7-bin/bin/composer config -g repos.packagist composer https://packagist.jp
RUN /opt/php-7-bin/bin/php /opt/php-7-bin/bin/composer config -g secure-http false

# Install Guzzle, prepare vendor files
RUN mkdir /lambda-php-vendor && \
    cd /lambda-php-vendor && \
    /opt/php-7-bin/bin/php /opt/php-7-bin/bin/composer require guzzlehttp/guzzle && \
    /opt/php-7-bin/bin/php /opt/php-7-bin/bin/composer require aws/aws-sdk-php

###### Create runtime image ######

FROM lambci/lambda:provided as runtime

# Layer 1
COPY --from=builder /lambda-php-runtime /opt/

# Layer 2
COPY --from=builder /lambda-php-vendor/vendor /opt/vendor

###### Create function image ######

FROM runtime as function

COPY function/hello /var/task/src/
#+END_SRC

    続いて、docker-compose.yamlを作成します。コンテナ一つでも楽なのでいつもdocker-compose使ってます。

#+BEGIN_SRC yaml
#+CAPTION: docker-compose.yaml
version: '3'
services:
  lambda_hello:
    build: .
    tty: true
    working_dir: /var/task/src
    ports:
      - 9001:9001
    volumes:
      - ./function/hello:/var/task/src:delegated
    environment:
      DOCKER_LAMBDA_WATCH: 1
      DOCKER_LAMBDA_STAY_OPEN: 1
      DOCKER_LAMBDA_API_PORT: 9001
      TEST_ENV_VAR: "hello world!"
    command: hello
#+END_SRC

     function/hello/hello.phpを作成して、Lambda関数本体を作成します。環境変数からデータを取得して返却するだけの処理です。

#+BEGIN_SRC php
#+CAPTION: function/hello/hello.php
<?php

function hello($data)
{
    $data = json_decode($data['body'], true);
    $text = getenv('TEST_ENV_VAR');
    $param = (isset($data['param'])) ? $data['param'] : '山田 太郎'; 
    $response = [
        'statusCode' => 200,
        'body' => $text . ' ' . $param . 'さん',
    ];
    return $response;
}
#+END_SRC

     次に、runtime/bootstrapを作成します。これは、Lambdaで取得したリクエストを取得して、パラメータをhandler（今回はhello.php）に渡し、返却されたデータをレスポンスとして返却しています。

     http通信周りはGuzzleを使用しているので、コード自体は44step程度しかないです。

#+BEGIN_SRC php
#+CAPTION: runtime/bootstrap
#!/opt/bin/php
<?php

// Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0

// This invokes Composer's autoloader so that we'll be able to use Guzzle and any other 3rd party libraries we need.
require __DIR__ . '/vendor/autoload.php';

function getNextRequest()
{
    $client = new \GuzzleHttp\Client();
    $response = $client->get('http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/next');

    return [
      'invocationId' => $response->getHeader('Lambda-Runtime-Aws-Request-Id')[0],
      'payload' => json_decode((string) $response->getBody(), true)
    ];
}

function sendResponse($invocationId, $response)
{
    $client = new \GuzzleHttp\Client();
    $client->post(
      'http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/' . $invocationId . '/response',
      ['body' => json_encode($response)]
    );
}

// This is the request processing loop. Barring unrecoverable failure, this loop runs until the environment shuts down.
do {
    // Ask the runtime API for a request to handle.
    $request = getNextRequest();

    // Obtain the function name from the _HANDLER environment variable and ensure the function's code is available.
    $handlerFunction = array_slice(explode('.', $_ENV['_HANDLER']), -1)[0];
    require_once $_ENV['LAMBDA_TASK_ROOT'] . '/src/' . $handlerFunction . '.php';

    // Execute the desired function and obtain the response.
    $response = $handlerFunction($request['payload']);

    // Submit the response back to the runtime API.
    sendResponse($request['invocationId'], $response);
} while (true);
#+END_SRC

     さて、これでdocker-composeを起動してみます。

#+BEGIN_SRC bash
#+CAPTION: docker-compose起動
$ docker-compose up -d
#+END_SRC

     PHPをソースからビルドするので初回起動のイメージビルド時は若干時間がかかります。

     コンテナが起動したら、軽く動作確認します。

#+begin_src bash
#+caption: 動作確認
$ curl -d '{}' http://localhost:9001/2015-03-31/functions/hello/invocations
{"statusCode":200,"body":"hello world! \u5c71\u7530 \u592a\u90ce\u3055\u3093"}
#+end_src

     ちゃんと期待通り動作してますね。ちょっと補足ですが、デプロイ先の環境はAPI Gatewayのプロキシ統合を使用するので、レスポンスパターンはきちんと合わせます。

     ローカルだとresponseCodeを400とかに設定してもAPIを叩くと200になってしまいますが、デプロイされるときちんと400になります（このあたりの差分をローカルでもきちんと吸収したいけどイマイチ解決策が見つからず・・・）。

     また、リクエストパラメータについても、API Gatewayを通るとJSONオブジェクトのbodyパラメータにテキストでエンドユーザが送ったパラメータが格納されるので注意です。
     
**** img2lambdaでデプロイパッケージを作成する
     さて、ここまでくると下記のようなディレクトリ構成になっているかと思います。

     #+begin_src plantuml :file overview.svg :cache yes :cmdline -config "$HOME/.emacs.d/styles.uml" :async
@@startsalt
{
  {T
   + project_root
   ++ Dockerfile
   ++ docker-compose.yaml
   ++ function
   +++ hello
   ++++ hello.php
   ++ runtime
   +++ bootstrap
  }
}
@@endsalt
     #+end_src

     #+caption: 現在のディレクトリ構成
     #+RESULTS[d80c487c05e65b3c3eb35f5a261f4ced9448a97f]:
     [[file:overview.svg]]

     インストールは、[[https://github.com/awslabs/aws-lambda-container-image-converter/releases][GithubのReleaseページ]] から、それぞれのプラットフォーム用のバイナリをダウンロードしてパスが通ってるところに配置するだけです。

#+begin_src bash
#+caption: img2lambdaのインストール確認
$ img2lambda --version
img2lambda version 1.2.4 (1d7760a)
#+end_src

     これでローカルのDockerコンテナをデプロイパッケージに固める準備ができたので、下記のコマンドをプロジェクトルートディレクトリで実行します。

#+begin_src bash
$ img2lambda -i lambci_lambda_hello:latest -r ap-northeast-1 -o ./output
2020/09/02 08:58:29 Parsing the image docker-daemon:lambci_lambda_hello:latest
2020/09/02 08:58:58 Image docker-daemon:lambci_lambda_hello:latest has 5 layers
...
2020/09/02 09:00:07 Lambda layer ARNs (2 total) are written to output/layers.json and output/layers.yaml
#+end_src

**** serverlessで自動デプロイする
     デプロイパッケージの準備ができたので、serverlessで自動デプロイします。

     deploy/serverless.ymlを作成し、下記のように記述します。

#+begin_src yaml
service: Lambda

provider:
  name: aws
  runtime: provided
  region: ap-northeast-1

package:
  individually: true

functions:
  LambciHello:
    handler: hello
    package:
      artifact: ../output/function.zip
    layers:
      ${file(../output/layers.json)}
    events:
      - http:
          path: /hello
          method: post
    environment:
      TEST_ENV_VAR: "hello from lambda!"
#+end_src

     それではデプロイしてみます（ ~aws configure~ とかはやっといてね）。

#+begin_src bash
$ cd deploy
$ sls deploy
#+end_src

     正常に完了すると、きちんとLambdaとAPI Gatewayが作成されていることがわかります。

#+DOWNLOADED: clipboard @ 2020-09-02 09:13:37
#+CAPTION: デプロイ結果
[[file:blog.org_imgs/20200902_091337.png]]

     実際にPOSTしてみると、たしかに期待通りのレスポンスが返ってきています。

#+begin_src bash
$ curl -d '{}' https://*********.execute-api.ap-northeast-1.amazonaws.com/dev/hello
hello from lambda! 山田 太郎さん

$ curl -d '{"param": "田中 次郎"}' https://*********.execute-api.ap-northeast-1.amazonaws.com/dev/hello
hello from lambda! 田中 次郎さん
#+end_src

**** おわりに
     今の所上記のようなフローで開発が進めていますが、ローカルでAPI Gatewayのプロキシ統合がシミュレートできない点についてはもう少し改善の余地があるかなーと思います。

     現状、環境によって下記のようにエスケープしないといけないので・・・。

***** ローカル
#+begin_src json
{
    "body": "{\r\n\"param\": \"aiueo\"\r\n}"
}
#+end_src

***** Lambda
#+begin_src json
{
    "param": "aiueo"
}
#+end_src

     まあ、そこを除けば良いフローかと思います。

     そのうち、SAMのローカル環境とかも試してみたい。
** DONE 【Rust】as_bytes()でcannot borrow as mutable(E0596)エラー :Rust:trouble_shooting:
   :PROPERTIES:
   :EXPORT_FILE_NAME: rust-array-from-as-bytes-mutable-error
   :EXPORT_HUGO_SECTION: /posts/2020/10
   :EXPORT_DATE: 2020-10-30
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
*** cannot borrow data in a `&` reference as mutable
    共通鍵関連で、DES暗号化をRustで実装しているんですが、そのときにちょっとハマりかけたのでメモ。
    <!--more-->

#+CAPTION: 問題となったコード
#+BEGIN_SRC rust
fn main() {
    let mut src = "abc".to_string();
    let mut s = src.as_bytes();
    
    println!("{:08b}", &s[0]);
    set_bit(&mut s, 0);
    println!("↓");
    println!("{:08b}", &s[0]);
}

fn set_bit(bytes: &mut [u8], bit: usize) {
    bytes[bit / 8 as usize] |= 0x80 >> (bit % 8);
}
#+END_SRC

処理自体は単純で、文字列をbyte配列に変換後、指定されたビットを立てるような感じです。

ただ、このソースをコンパイルしようとすると、下記のようなエラーが発生します。

#+caption: エラー内容
#+BEGIN_SRC
error[E0596]: cannot borrow data in a `&` reference as mutable
 --> src/main.rs:8:13
  |
8 |     set_bit(&mut s, 0);
  |             ^^^^^^ cannot borrow as mutable
#+END_SRC

*** 原因
    [[https://moshg.github.io/rust-std-ja/std/primitive.str.html#method.as_bytes][as_bytes()の定義]]を確認すると、

#+BEGIN_QUOTE
pub const fn as_bytes(&self) -> &[u8]
#+END_QUOTE
    
    ~as_bytes()~ の返り値はバイト配列への不変参照になるので、それを可変な変数に格納しても、 ~set_bit()~ でバイト配列には可変アクセスできないのでした。

#+DOWNLOADED: clipboard @ 2020-10-30 09:45:37
#+CAPTION: as_bytes()とas_bytes_mut()の違い
[[file:blog.org_imgs/20201030_094537.png]]
    
*** 解決策
    今回の場合、unsafeなas_bytes_mut()を使用することで、バイト配列への可変参照を取得できます。

    最終的には下記のようなソースにすることで、コンパイルが通ります。

#+CAPTION: 修正したコード
#+BEGIN_SRC rust 
fn main() {
    let mut src = "abc".to_string();
    let mut s = unsafe { src.as_bytes_mut() };
    
    println!("{:08b}", &s[0]);
    set_bit(&mut s, 0);
    println!("↓");
    println!("{:08b}", &s[0]);
}

fn set_bit(bytes: &mut [u8], bit: usize) {
    bytes[bit / 8 as usize] |= 0x80 >> (bit % 8);
}
#+END_SRC

出力結果：

#+caption: 出力結果
#+BEGIN_SRC
01100001
↓
11100001
#+END_SRC

*** まとめ
    Rustを書き始めたときはちんぷんかんぷんでしたが、最近はメモリの状態を意識しながら書くことに慣れてきました。こういうしょうもないエラーもたまにやってしまいますが、トラブルシューティングはかなりスムーズにできるようになってきた気がします。

** DONE 旧ブログの記事をこちらに移行しました                          :Other:
   :PROPERTIES:
   :EXPORT_FILE_NAME: blog-migrated-from-wordpress-to-hugo
   :EXPORT_HUGO_SECTION: /posts/2020/11
   :EXPORT_DATE: 2020-11-10
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
   今回はちょっとしたお知らせです。

   随分前からzeroclock.devを始めてはいたのですが、旧サイトの[[https://vivolog.net][ビボログ]]の記事はほったらかしだったのでこっちに持ってきました。

   +XServer高いので早く呪縛から開放されたかった+

   <!--more-->

   当サイトはHugoとgithub.ioの組み合わせで、旧サイトはWordpressだったのでどうしたもんかなーと思い、色々調べてみたら[[https://github.com/SchumacherFM/wordpress-to-hugo-exporter][wordpress-to-hugo-exporter]]というプラグインを使ったらサクッと移行できました。

   ざっと手順はこんな感じ。

#+BEGIN_QUOTE
1. 上記のgithubページからソースを丸ごとzipでダウンロード
2. Wordpress管理画面から、ダウンロードしたzipを読み込んでプラグインを追加
3. ツール＞Export to Hugoで丸ごとダウンロードされる
#+END_QUOTE
   
   いらない画像とかも全部入ってるので、必要なものをピックアップしつつ、マークダウンも若干おかしいので手直ししつつ〜という感じで、そこまで苦労せずに移行できました。

   ただ、記事数が多かったり画像が超多かったりするとメモリ不足で死ぬらしいので、その場合は下記の記事を参考に・・・。

   [[https://randd.kwappa.net/2020/05/17/migrate-wordpress-to-hugo-and-netlify/][WordPressのBlogをHugoとNetlifyに移行する]]

   移行完了したので、旧サイトは多分11月下旬あたりに閉鎖すると思われます。

   
   ということで、今回はちょっとしたお知らせでした。
** DONE Lambdaのローカル開発環境とCI/CD構築（coverageも） :Rust:Github:TravisCI:Codecov:Lambda:AWS:rusoto:
   :PROPERTIES:
   :EXPORT_FILE_NAME: lambda-local-and-continuous-deployment
   :EXPORT_HUGO_SECTION: /posts/2020/11
   :EXPORT_DATE: 2020-11-15
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
*** Lambdaをサクサク作りたい
    最近実務でもプライベートでもLambdaを使う機会が多いのですが、毎回悩むのが開発環境とCI/CD。

    ちょっとしたLambdaならブラウザコンソール上のエディタを使って作るとか、zipで固めるとかでいいんですけど、それなりに大きなLambda関数だとやっぱり

#+BEGIN_QUOTE
ローカルで開発＆単体テスト＆結合テスト --> GitにPush --> 自動テスト＆デプロイ（ついでにcoverage計測）
#+END_QUOTE

    てな流れを作りたい。

<!--more-->

**** 構成
    
    ということで、今回は下記のような構成で作ってみたいと思います。

#+DOWNLOADED: clipboard @ 2020-11-15 09:01:25
#+CAPTION: 構成図
[[file:blog.org_imgs/20201115_090125.png]]


#+BEGIN_QUOTE
1. ローカルでコーディング、テスト、動作確認
2. GitにPush（masterブランチはdev環境、releaseブランチはrelease環境）
3. 自動テスト、デプロイ、codecovにカバレッジ送信
4. プルリクの場合、codecovからプルリクにカバレッジレポート自動POST
#+END_QUOTE

    開発者がやることはローカルでコーディングしてテスト書いてGitにPushするだけ。あとは自動でテスト、カバレッジ計測、問題無ければデプロイまで実施してくれるようにしたいです。

    今回作るAPIは下記の通りです。S3との通信ができれば他サービスとの連携も可能なので、これくらいシンプルで良いと思います。

#+begin_quote
Request(application/json): { "textBody": "ファイルに書き込みたい内容" }

Response(ok)：{ "messasge": "Succeeded." }

Response(4xx/5xx)：{ エラー内容 }

Description： ~textBody~ に指定した内容をS3バケットの ~text.txt~ に保存するAPI。
#+end_quote

**** 使用する技術・サービス
     今回使用する技術・サービスのうち、主要なものを記載しておきます。

#+CAPTION: 主要な技術・サービス一覧
| 技術・サービス名       | 概要                                                                                                                              |
| Serverless Framework   | AWSの各種サービスへのデプロイを自動化するためのフレームワーク。 ~serverless.yml~ に構成を記述することで、一発でデプロイできます。 |
| serverless-offline  | ローカルでserverlessでデプロイする環境のうち、LambdaとAPI Gatewayを再現するプラグイン。                                           |
| serverless-s3-local | ローカルでserverlessでデプロイする環境のうち、S3を再現するプラグイン。serverless-offlineのプラグインという位置付けです。          |
| serverless-rust   | serverless-offlineでRustで書いたLambdaを動作させるためのプラグイン。                                                              |
| TravisCI               | CIサービス。公開リポジトリなら無料で使えます。                                                                                    |
| Codecov                | カバレッジレポートサービス。公開リポジトリなら無料で使えます。                                                                                          |

     TravisCIとCodecovの登録方法については割愛するので、未登録の方は登録しておいてください（Githubでログインするだけ）。

---

*** ローカル開発環境とメイン処理
    それでは始めにローカル開発環境の構築を行っていきます。S3との通信はとりあえず置いといて、APIにPOSTしたらバリデーションして返答を返すところまで。

**** プロジェクト作成
    今回は[[https://github.com/softprops/serverless-aws-rust-http][serverless AWS Rust HTTP template]]をベースにプロジェクトを作成します。

#+caption: プロジェクト作成コマンド
#+begin_src bash
$ npx sls install \
  --url https://github.com/softprops/serverless-aws-rust-http \
  --name lambda-rust-sample
$ cd lambda-rust-sample
#+end_src

**** 必要なnode modulesの追加
#+caption: 必要なnode_modulesのインストール
#+begin_src bash
$ npm i serverless -g
$ npm i -D serverless-offline serverless-s3-local serverless-rust
#+end_src

      なお、serverlessコマンドはglobal領域にインストールしておきます。

      また、serverless-offlineのmasterブランチは本記事執筆時点（2020/11/15）でrustに対応しておらず、[[https://github.com/dherault/serverless-offline/pull/1059][こちらのプルリクエスト]]で対応されているので、そっちを使うようにします。

#+caption: package.jsonの修正
#+begin_src json
    "serverless-offline": "EgorDm/serverless-offline.git#feature/rust-invoke",
#+end_src

**** serverless.ymlの修正
      ~serverless.yml~ を修正して、下記の通り修正します。

#+caption: serverless.yml
#+begin_src yml
service: rust-lambda-sample
provider:
  name: aws
  runtime: rust
  memorySize: 128
  region: ap-northeast-1
  stage: ${opt:stage, self:custom.defaultStage}
  logs:
    restApi:
      accessLogging: true
package:
  individually: true
plugins:
  - serverless-rust
  - serverless-offline
  - serverless-s3-local
functions:
  hello:
    handler: hello
    events:
      - http:
          path: '/'
          method: POST
          integration: lambda
          request:
            template:
              application/json: $input.json('$')
custom:
  s3:
    host: localhost
    directory: /tmp
    port: 8000
    vhostBuckets: false
#+end_src

      少しポイントになる所を解説。

#+caption: [point1] plugins
#+begin_src yml
...
provider:
  name: aws
  runtime: rust
plugins:
  - serverless-rust
  - serverless-offline
  - serverless-s3-local
...
#+end_src

      ここで、プラグインとして先程追加したserverless-rust、offline、s3-localを使用することを示しています。また、serverless-rustのおかげでruntimeとしてrustを指定できます。

#+caption: [point2] ingeration
#+begin_src yml
...
functions:
  hello:
    handler: hello
    events:
      - http:
          path: '/'
          method: POST
          integration: lambda
          request:
            template:
              application/json: $input.json('$')
...
#+end_src

      ~integration~ に ~lambda~ を指定しています。何も指定しないと ~lambda-proxy~ となりますが、プロキシ統合だと勝手にAPI Gatewayでリクエストとレスポンスのマッピングな行われてしまい上手くいかなかったのでLambda統合にしました。

      リクエストマッピングについては ~template~ にて設定しており、リクエストの ~body~ だけ取り出してAPI GatewayからLambdaにパスする流れになっています。

**** Cargo.tomlの修正
      ひとまず必要な依存関係だけ定義しておきます。 ~anyhow~ や ~simple_logger~ はそれぞれエラーハンドリングとログパッケージですが、お好きなパッケージがありましたらそちらを使用してもOKです。

      なお、 ~lambda~ と ~lambda_http~ についてはcrates.ioに上がっているパッケージではなく、githubの最新ソースから取得するようにします。

#+caption: Cargo.toml
#+begin_src toml
[package]
name = "hello"
version = "0.1.0"
edition = "2018"

[dependencies]
tokio = { version = "0.2", features = ["macros"] }
lambda = { git = "https://github.com/awslabs/aws-lambda-rust-runtime/", branch = "master"}
lambda_http = { git = "https://github.com/awslabs/aws-lambda-rust-runtime/", branch = "master"}
serde_derive = "1.0.117"
serde = "1.0.117"
serde_json = "1.0.59"
simple_logger = "1.11.0"
log = "0.4.11"
anyhow = "1.0.34"
#+end_src
      
**** Lambda本体の作成
      LambdaのソースコードをRustで記述します。

#+caption: main.rs
#+begin_src rust
use lambda::{handler_fn, Context};
use anyhow::{anyhow, Result};
use serde_derive::{Deserialize, Serialize};
use simple_logger::SimpleLogger;
use log::{LevelFilter, error};

#[derive(Deserialize, Debug)]
#[serde(rename_all = "camelCase")]
struct CustomEvent {
    text_body: Option<String>,
}

#[derive(Serialize, Debug, PartialEq)]
struct CustomOutput {
    message: String,
}

const MSG_EMPTY_TEXT_BODY: &str = "Empty text body.";
const MSG_TEXT_BODY_TOO_LONG: &str = "Text body is too long (max: 100)";

#[tokio::main]
async fn main() -> Result<()> {
    SimpleLogger::new().with_level(LevelFilter::Debug).init().unwrap();
    lambda::run(handler_fn(hello))
        .await
        // https://github.com/dtolnay/anyhow/issues/35
        .map_err(|err| anyhow!(err))?;
    Ok(())
}

async fn hello(event: CustomEvent, c: Context) -> Result<CustomOutput> {
    if let None = event.text_body {
        error!("Empty text body in request {}", c.request_id);
        return Err(anyhow!(get_err_msg(400, MSG_EMPTY_TEXT_BODY)));
    }
    let text = event.text_body.unwrap();
    if text.len() > 100 {
        error!("text body is too long (max: 100) in request {}", c.request_id);
        return Err(anyhow!(get_err_msg(400, MSG_TEXT_BODY_TOO_LONG)));
    }
    
    Ok(CustomOutput {
        message: format!("Succeeded.")
    })
}

fn get_err_msg(code: u16, msg: &str) -> String {
    format!("[{}] {}", code, msg)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn can_hello_handler_handle_valid_request() {
        let event = CustomEvent {
            text_body: Some("Firstname".to_string())
        };
        let expected = CustomOutput {
            message: "Succeeded.".to_string()
        };
        assert_eq!(
            hello(event, Context::default())
                .await
                .expect("expected Ok(_) value"),
            expected
        )
    }

    #[tokio::test]
    async fn can_hello_handler_handle_empty_text_body() {
        let event = CustomEvent {
            text_body: None
        };
        let result = hello(event, Context::default()).await;
        assert!(result.is_err());
        if let Err(error) = result {
            assert_eq!(
                error.to_string(),
                format!("[400] {}", MSG_EMPTY_TEXT_BODY)
            )
        } else {
            // result must be Err
            panic!()
        }
    }

    #[tokio::test]
    async fn can_hello_handler_handle_text_body_too_long() {
        let event = CustomEvent {
            text_body: Some("12345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901".to_owned())
        };
        let result = hello(event, Context::default()).await;
        assert!(result.is_err());
        if let Err(error) = result {
            assert_eq!(
                error.to_string(),
                format!("[400] {}", MSG_TEXT_BODY_TOO_LONG)
            )
        } else {
            // result must be Err
            panic!()
        }
    }
}
#+end_src

      ~CustomEvent~ がAPI Gatewayから受け取った内容（＝ユーザのリクエストbody）に対応しており、 ~text_body~ を ~Option~ にすることで何も指定しない場合に500で死なないようにしています。

      ~CustomOutput~ はレスポンスの内容になります。

      ややこしいことは ~lambda-rust-runtime~ がやってくれるので、こちらが書く内容としてはリクエストとレスポンスをマッピングする構造体を定義して、それを返却するだけです。現状のソースでは、リクエストに ~textBody~ が存在すれば ~Succeeded.~ が、存在しなかったり100文字以上だとエラーメッセージが返ってきます。

**** 動作確認
      それでは、ローカルで動作確認してみます。

      プロジェクトルートで下記のコマンドを実行し、ローカル環境を走らせます。

#+caption: serverless開始コマンド
#+begin_src bash
$ npm i
$ sls offline start --stage local
...
POST | http://localhost:3000/local
#+end_src
      
      この状態で、POSTを飛ばしてみます。POSTした時点でRustのビルドがスタートするので、初回は結構時間がかかります。

#+caption: POSTしてみる
#+begin_src bash
$ curl -X POST -H "Content-Type: application/json" -d '{"textBody": "aaaaa"}' http://localhost:3000/local
{"body":"{\"message\":\"Succeeded.\"}"}
$ curl -X POST -H "Content-Type: application/json" -d '{"textBodyyyyy": "aaaaa"}' http://localhost:3000/local
{"body":"{\"errorType\":\"anyhow::Error\",\"errorMessage\":\"[400] Empty text body.\"}"}
#+end_src

      ローカルだとresponseがbodyに入っちゃってますが、実際にデプロイすると中身だけちゃんと返ってきます（ほんとは同じ挙動になってほしいけど多分serverless-offlineの仕様orバグ？）。

**** デプロイ確認
      ここまでのソースで、手動でデプロイできるか確認しておきます。

      serverlessのcredential設定を行います。

#+caption: serverlessのcredential設定＆デプロイ実行
#+begin_src bash
$ sls config credentials --stage dev --provider aws --key "${AWS_ACCESS_KEY_ID}" --secret "${AWS_SECRET_ACCESS_KEY}"
$ sls deploy --stage dev
#+end_src

#+DOWNLOADED: clipboard @ 2020-11-15 13:27:53
#+CAPTION: デプロイ結果 
[[file:blog.org_imgs/20201115_132753.png]]

      デプロイ後、表示されたエンドポイントにPOSTしてみて、想定通りのレスポンスが返却されることを確認します。

---
*** S3通信処理の追加とカバレッジ収集
**** S3との通信処理実装
     S3との通信に使用するcrateは[[https://github.com/rusoto/rusoto][rusoto]]です。

     まず、依存関係を追加します。

#+caption: Cargo.tomlへの追加内容
#+begin_src toml
rusoto_core = "0.45.0"
rusoto_s3 = "0.45.0"
rusoto_mock = "0.45.0"
#+end_src

     今回使用するバケットの情報を ~serverless.yml~ に追加しておきます。

#+caption: serverless.ymlへのバケット情報追加
#+begin_src yml
service: rust-lambda-test
provider:
...
  iamRoleStatements:
    - Effect: "Allow"
      Action:
        - "s3:*"
      Resource:
        Fn::Join:
          - ""
          - - "arn:aws:s3:::"
            - ${self:custom.bucketName.${self:provider.stage}}
            - "/*"
...
functions:
  hello:
...
    environment:
      BUCKET_NAME: ${self:custom.bucketName.${self:provider.stage}}
      LOCAL_FLAG: ${self:custom.localFlag.${self:provider.stage}}
resources:
  Resources:
    Bucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:custom.bucketName.${self:provider.stage}}
custom:
...
  bucketName:
    local: zeroclock-lambda-rust-bucket-local
    dev: zeroclock-lambda-rust-bucket-dev
    release: zeroclock-lambda-rust-bucket-release
  localFlag:
    local: local
    dev: ''
    release: ''
#+end_src
     
     S3Clientを取得する処理と、単体テストを追加します。すいません、ちょっと長いです・・・。

#+caption: main.rs
#+begin_src rust
use std::env;
use rusoto_s3::{
    S3,
    S3Client,
    PutObjectRequest,
};
use rusoto_core::Region;
use rusoto_mock::{
    MockCredentialsProvider,
    MockRequestDispatcher,
    MockResponseReader,
    ReadMockResponse,
};

...
const MOCK_KEY: &str = "AWS_MOCK_FLAG";
const BUCKET_NAME_KEY: &str = "BUCKET_NAME";
const LOCAL_KEY: &str = "LOCAL_FLAG";

...
async fn hello(event: CustomEvent, c: Context) -> Result<CustomOutput> {
...
    let s3 = get_s3_client();
    let bucket_name = env::var(BUCKET_NAME_KEY)?;
    s3.put_object(PutObjectRequest {
        bucket: bucket_name.to_string(),
        key: "test.txt".to_string(),
        body: Some(text.into_bytes().into()),
        acl: Some("public-read".to_string()),
        ..Default::default()
    }).await?;
    
    Ok(CustomOutput {
        message: format!("Succeeded.")
    })
}

...
fn get_s3_client() -> S3Client {
    let s3 = match env::var(MOCK_KEY) {
        Ok(_) => {
            // Unit Test
            S3Client::new_with(
                MockRequestDispatcher::default().with_body(
                    &MockResponseReader::read_response("mock_data", "s3_test.json")
                ),
                MockCredentialsProvider,
                Default::default(),
            )
        },
        Err(_) => {
            if env::var(LOCAL_KEY).unwrap() != "" {
                // local
                return S3Client::new(Region::Custom {
                    name: "ap-northeast-1".to_owned(),
                    endpoint: "http://host.docker.internal:8000".to_owned(),
                })
            }
            // cloud
            return S3Client::new(Region::ApNortheast1)
        },
    };
    s3
}

#[cfg(test)]
mod tests {
    use super::*;

    fn setup() {
        env::set_var(MOCK_KEY, "1");
        env::set_var(BUCKET_NAME_KEY, "test-bucket");
    }

    #[test]
    fn can_get_local_s3_client() {
        env::set_var(LOCAL_KEY, "local");
        let _s3 = get_s3_client();
        assert!(true);
    }

    #[test]
    fn can_get_cloud_s3_client() {
        env::set_var(LOCAL_KEY, "");
        let _s3 = get_s3_client();
        assert!(true);
    }

    #[tokio::test]
    async fn can_hello_handler_handle_valid_request() {
        setup();
        let event = CustomEvent {
            text_body: Some("Firstname".to_string())
        };
        let expected = CustomOutput {
            message: "Succeeded.".to_string()
        };
        assert_eq!(
            hello(event, Context::default())
                .await
                .expect("expected Ok(_) value"),
            expected
        )
    }

    #[tokio::test]
    async fn can_hello_handler_handle_empty_text_body() {
        setup();
        let event = CustomEvent {
            text_body: None
        };
        let result = hello(event, Context::default()).await;
        assert!(result.is_err());
        if let Err(error) = result {
            assert_eq!(
                error.to_string(),
                format!("[400] {}", MSG_EMPTY_TEXT_BODY)
            )
        } else {
            // result must be Err
            panic!()
        }
    }

    #[tokio::test]
    async fn can_hello_handler_handle_text_body_too_long() {
        setup();
        let event = CustomEvent {
            text_body: Some("12345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901".to_owned())
        };
        let result = hello(event, Context::default()).await;
        assert!(result.is_err());
        if let Err(error) = result {
            assert_eq!(
                error.to_string(),
                format!("[400] {}", MSG_TEXT_BODY_TOO_LONG)
            )
        } else {
            // result must be Err
            panic!()
        }
    }
}
#+end_src
     
     単体テスト時はrusoto_mockを使用し、ローカル開発環境の場合はカスタムエンドポイントで生成しています。 ~host.docker.internal~ は、Dockerコンテナから見たホストマシンのIPアドレスです（serverless-offlineのrustプラグインの場合、内部的にdockerが起動しているため）。

     なお、credentialsは環境変数を使用するのでコード内には出てきません。

     rusoto_mockでS3Clientを生成する際、レスポンスのデータを記述したファイルが必要なので、今回は空データを準備しておき、テストを実行してみます。

#+caption: テスト実行
#+begin_src bash
$ mkdir mock_data
$ touch mock_data/s3_test.json
$ cargo test
    Finished test [unoptimized + debuginfo] target(s) in 5.07s
     Running target/debug/deps/hello-ed9968ea3f56ae48

running 5 tests
test tests::can_get_cloud_s3_client ... ok
test tests::can_get_local_s3_client ... ok
test tests::can_hello_handler_handle_text_body_too_long ... ok
test tests::can_hello_handler_handle_empty_text_body ... ok
test tests::can_hello_handler_handle_valid_request ... ok

test result: ok. 5 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
#+end_src

**** カバレッジ収集
     カバレッジ収集は下記のCLIツールを使用します。

#+caption: カバレッジ収集に使用するCLIツール
| 名称        | 概要                                                                                                                                                       |
| lcov        | カバレッジデータ自体は後述のgrcovで良いのですが、ローカルでカバレッジをHTMLに出力するために使用（genhtmlコマンド）。Macの場合は ~brew install lcov~ でOK。 |
| grcov       | Rustのカバレッジ収集ツール。Mozillaが保守しているので安心。Cargoでインストール( ~cargo install grcov~ )。                                                  |
| rust-covfix | 必須じゃないですけど、これが無いとなんかカバレッジが明らかに高かったり低かったり謎の現象に見舞われたので使用。Cargoでインストール（ ~cargo install rust-covfix~ ）。                                  |

     若干面倒なので、スクリプトを書いてプロジェクトルートに置いておきます（codecovのところは今はスルーでOK）。

#+caption: カバレッジ集計スクリプト（coverage.sh）
#+begin_src sh
#!/usr/bin/env bash

set -eux

PROJ_NAME=$(cat Cargo.toml | grep -E "^name" | sed -E 's/name[[:space:]]=[[:space:]]"(.*)"/\1/g' | sed -E 's/-/_/g')
rm -rf target/debug/deps/${PROJ_NAME}-*

export CARGO_INCREMENTAL=0
export RUSTFLAGS="-Zprofile -Ccodegen-units=1 -Copt-level=0 -Clink-dead-code -Coverflow-checks=off -Zpanic_abort_tests -C panic=abort"

cargo +nightly build
cargo +nightly test

zip -0 ccov.zip `find . \( -name "${PROJ_NAME}*.gc*" -o -name "test-*.gc*" \) -print`
grcov ccov.zip -s . -t lcov --llvm --branch --ignore-not-existing --ignore "/*" --ignore "tests/*" -o lcov.info
rust-covfix -o lcov.info lcov.info

if [ $# = 0 ] || [ "$1" != "ontravis" ]; then
    genhtml -o report/ --show-details --highlight --ignore-errors source --legend lcov.info --branch-coverage
fi

if [ $# -gt 1 ] && [ "$2" = "sendcov" ]; then
    bash <(curl -s https://codecov.io/bash) -f lcov.info -t "${CODECOV_TOKEN}"
fi
#+end_src
     
     引数無しで、カバレッジを計測してレポートをHTML出力させます。

#+begin_src bash
$ bash coverage.sh
#+end_src

     成功したら、 ~report/index.html~ をブラウザで開くとカバレッジが見れます。

#+DOWNLOADED: clipboard @ 2020-11-15 17:34:04
#+CAPTION: カバレッジレポート表示例 
[[file:blog.org_imgs/20201115_173404.png]]

     これで、カバレッジ計測までいけました。

**** ローカルでS3との連携テスト
     単体テストも通ってカバレッジも取れるようになったので、S3連携処理込みでローカル動作確認してみます。

     はじめに、serverless-s3-local用にAWS CLI用のcredentials設定が必要です。

#+caption: AWS Credentials設定
#+begin_src bash
$ vim ~/.aws/credentials
#以下を追加
[s3local]
aws_access_key_id=S3RVER
aws_secret_access_key=S3RVER
#+end_src

#+caption: ローカルで確認
#+begin_src bash
# タブA
# AWS_PROFILEにs3-local用のprofileを指定
$ AWS_PROFILE=s3local sls offline start --stage local
# タブB
$ curl -X POST -H "Content-Type: application/json" -d '{"textBody": "aaaaa"}' http://localhost:3000/local
{"body":"{\"message\":\"Succeeded.\"}"}
#+end_src

     成功したっぽいので、実際に保存されているか確認してみます。

#+caption: ローカルのS3確認
#+begin_src bash
$ aws --endpoint="http://localhost:8000" s3 cp s3://zeroclock-lambda-rust-sample-bucket-local/test.txt /tmp/s3_result.txt --profile s3local
$ cat /tmp/s3_result.txt
aaaaa
#+end_src

     いい感じ。

     最後に、手動デプロイを再度実行して問題なく完了することを確認します。

#+caption: 手動デプロイ
#+begin_src bash
$ sls deploy --stage dev
#+end_src

*** CI/CD環境構築
    コーディングしてローカルで検証して手動でデプロイするところまでは問題無かったので、最後に自動デプロイ＆カバレッジレポートを設定します。

    それぞれ、TravisCIとCodecovを使用しますが、それぞれの連携方法及びCLIツールのインストール方法については割愛します。Githubでログインしてレポジトリを選ぶだけなので。

    まず、デプロイに必要な下記の情報を安全にTravisCIに渡せるように暗号化します。

    - AWS IAMユーザのアクセスキーID（aws_access_key_id）
    - AWS IAMユーザのシークレットアクセスキー（aws_secret_access_key）
    - Codecovのトークン（CODECOV_TOKEN）

#+caption: 環境変数の暗号化
#+begin_src bash
$ travis encrypt aws_access_key_id="xxxxx..."
$ travis encrypt aws_secret_access_key="xxxxx..."
$ travis encrypt CODECOV_TOKEN="XXXXXXXX-xxxx...."
#+end_src
    
    TravisCI用の設定ファイル ~.travis.yml~ を作成して設定します。 ~secret~ には先程暗号化した3つの環境変数の情報が入ります。

#+caption: .travis.yml
#+begin_src bash
language: rust
rust:
  - nightly
cache: cargo
install:
  - cargo install grcov rust-covfix
  - nvm install 12.14.1 --latest-npm
  - nvm alias default 12.14.1
  - npm install serverless -g
  - npm install
  - sls config credentials --stage dev --provider aws --key "${aws_access_key_id}" --secret "${aws_secret_access_key}"
before_script:
  - cargo test
script:
  - npm run coverage:ci-sendcov
  - if [ "$TRAVIS_BRANCH" = "master" ] && [ "$TRAVIS_PULL_REQUEST" = "false" ]; then echo "This is master which released to dev stage." && npm run deploy:dev; fi
  - if [ "$TRAVIS_BRANCH" = "release" ] && [ "$TRAVIS_PULL_REQUEST" = "false" ]; then echo "This is release which released to release stage." && npm run deploy:release; fi
env:
  global:
    - secure: "fugahuga..."
    - secure: "hogehoge..."
    - secure: "hogehoge..."
#+end_src
    
    ~package.json~ にscriptを追加しておきます。

#+caption: package.jsonへのscript追加
#+begin_src json
{
  "scripts": {
    "start": "AWS_PROFILE=s3local sls offline start --stage local",
    "deploy:dev": "sls deploy --stage dev",
    "deploy:release": "sls deploy --stage release",
    "coverage": "bash coverage.sh",
    "coverage:ci": "bash coverage.sh ontravis",
    "coverage:ci-sendcov": "bash coverage.sh ontravis sendcov"  
  },
  ...
}
#+end_src

    最後に、Codecov用の設定ファイル ~codecov.yml~ を作成します。

#+caption: codecov.yml
#+begin_src yml
codecov:
  require_ci_to_pass: yes

coverage:
  precision: 2
  round: down
  range: "70...100"

parsers:
  gcov:
    branch_detection:
      conditional: yes
      loop: yes
      method: no
      macro: no

comment: # See: https://docs.codecov.io/docs/pull-request-comments
  layout: "reach, diff, flags, files"
  behavior: default
  require_changes: no
  require_base: yes
  require_head: yes
#+end_src
    
    これで、PR時には自動的にカバレッジレポートをコメントしてくれるはずです。

---
    
*** 動作確認＋まとめ
   では、実際にmasterブランチとreleaseブランチにそれぞれPushしてみます。

#+DOWNLOADED: clipboard @ 2020-11-15 22:33:52
#+CAPTION: TravisCIの自動デプロイ様子
[[file:blog.org_imgs/20201115_223352.png]]
   
   念の為それぞれの環境でAPIを叩いてみます。

#+DOWNLOADED: clipboard @ 2020-11-15 23:08:32
#+CAPTION: 動作確認
[[file:blog.org_imgs/20201115_230831.png]]

   問題無さそうですね。

   では、プルリクしてみます。

#+DOWNLOADED: clipboard @ 2020-11-15 22:51:34
#+CAPTION: プルリク画面キャプチャ
[[file:blog.org_imgs/20201115_225134.png]]

   きちんとカバレッジレポートがコメントされています。

   なお、今回作ったサンプルは下記のリポジトリになります。

   [[https://github.com/zeroclock/lambda-rust-sample][zeroclock/lambda-rust-sample]]

---

現状CI/CDサイクル回すのに10分程度かかっちゃってますが、Dockerイメージをキャッシュするとかでもっと早くなりそうな気がしています。

また、ローカルとデプロイ後で若干レスポンスの形式が異なる（ローカルだとbody階層が増えちゃってる）ので、そこも要調整な感じですが、まだマージされていないプルリクを使用しているので、もしかしたらマージされる頃には直っているかも（私の設定ミスの可能性もあり）。

なにはともあれ、これで色々な意味で足かせになっていたLambdaのCI/CD環境が構築できました。同じような悩みを抱えている人に参考になれば幸いです。

** DONE AWS Cloudwatch LogsのCLIビューワを作った    :Rust:AWS:Cloudwatch:CLI:
   :PROPERTIES:
   :EXPORT_FILE_NAME: megane-cloudwatch-logs-viewer
   :EXPORT_HUGO_SECTION: /posts/2021/05
   :EXPORT_DATE: 2021-05-23
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

   ブログ更新サボってる間に色々ネタがたまってしまったのでちょこちょこ放出していきます。

   いくつか作ったツールなどありますので、しばらくはそのあたりの紹介だったり、最近開発リーダー（PLじゃないよ）周りの仕事でアプリ設計だったり新しめのFWを使ったりしてるのでその辺の知見等も書けたらと思います。

   とりあえず今回は作ったツールのお話。

   <!--more-->
   
*** モチベーション
    業務でも日常的にAWS Cloudwatch Logsを見るわけなんですが、

    - Tailできない
    - 複数のロググループを見るときに複数タブを開かないといけない
    - ロググループが増えてきたときにいちいち検索するのが面倒
    - そもそもWebで見るのが面倒

#+begin_quote
    そもそもWebで見るのが面倒
#+end_quote

    これについては、商用環境のAWSアカウントがIP制限付きのためログインの度にroleの引受（assume）をしないといけないというのがあります。
    
    （単純にIP制限だけだと、裏でAWS側でリソースとってくるような場合にエラーになったりするので、ロール切替時にIPチェックをするようにしているため）

    ...と、色々「ブラウザでやんなくてよくね？」って思うことが多くなったので、せっかくなので作ってみたという。

*** 作ったもの

    こういうのを作りました。

    [[https://github.com/zeroclock/megane][zeroclock/megane - GitHub]]

#+caption: スクリーンショット
    [[https://github.com/zeroclock/megane/raw/master/image/screenshot.png]]
    
    CLIでログをサクッと見れるツールになっています。

    導入方法だったり使い方は ~README~ に書いてあるのですが、簡単に特徴を並べると下記のような感じです。

    - ログのTailが可能
    - 最大4つまでロググループを表示
    - ログの折りたたみ＆展開
    - ロググループのインクリメンタルサーチ
    - 選択中のログ全文をクリップボードにコピー
    - リージョン、プロファイルの切り替え
    - AssumeRole対応

    今の所そこまで大した機能は無く、キーマップも微妙だったり色々荒削りではありますが、リリース時に複数サーバのログのTailを垂れ流したいときには使えるかなーと思います。

    一応実案件でも使ってもらっており、色々FBもらって調整中です。

*** 技術的な話

    言語はRustを使いました。ちょっと前からGoとかRust製のCLIツールが色々出てきたので、それに乗っかった形。

    （ ~peco~ とか ~gitui~ とか色々有り難く使わせてもらってます ）

    実装については、 ~tokio~ で非同期ゴリゴリです。UIについては[[https://github.com/fdehau/tui-rs][tui-rs]]を使用。

    下記のような非同期タスクをspawnして、お互いにchannelを通じてイベントをやりとりしてログの取得だったりキー入力を捌いたりしています。

    - InputEventHandler : キー入力や、Tick（画面描画タイミング）を監視
    - LogEventEventHandler : ログデータ関連のイベントを監視（ログ検索イベント、取得したログ削除イベント等）
    - LogGroupEventHandler : ロググループ関連のイベントを監視（ロググループ検索イベント等）
    - MainEventHandler : キー入力やTickイベントが発生した場合に画面の再描画やキーイベントのハンドリングを行うメイン処理
    - TailLogEventEventHandler : ログのTail関連のイベントを監視（Tail開始/停止イベント、TailのTickイベント等）

    UIについては下記のような ~Drawable~ トレイトを作って、それを実装するstructをUIパーツごとに作ってそれを組み合わせるといった感じにしています。

#+caption: Drawableトレイト
#+begin_src rust
#[async_trait]
pub trait Drawable<B>
where
    B: Backend,
{
    /// all components must be drawable
    fn draw(&mut self, f: &mut Frame<'_, B>, area: Rect);

    /// handles input key event
    /// and returns if parent component should handle other events or not
    async fn handle_event(&mut self, event: KeyEvent) -> bool;

    /// push the key mappings for this component
    fn push_key_maps<'a>(
        &self,
        maps: &'a mut BTreeMap<KeyEventWrapper, String>,
    ) -> &'a mut BTreeMap<KeyEventWrapper, String> {
        maps
    }
}
#+end_src
    
    それぞれ、画面描画処理（ ~draw()~ ）、キー入力イベントのハンドリング処理（ ~handle_event()~ ）、画面下部に表示されるキーマップのヘルプ情報の格納処理（ ~push_key_maps~ ）を実装するような構成にしました。

    例えば、コード量の少ない ~Help~ だとこんな感じ（ ~push_key_maps()~ はデフォルト実装を使用 ）。

#+caption: Help構造体
#+begin_src rust
#[async_trait]
impl<B> Drawable<B> for Help<B>
where
    B: Backend + Send,
{
    fn draw(&mut self, f: &mut Frame<'_, B>, area: Rect) {
        let block = Block::default()
            .title("HELP".to_string())
            .borders(Borders::ALL);
        let paragraph = Paragraph::new(self.msg.as_ref())
            .block(block)
            .wrap(Wrap { trim: false });
        f.render_widget(paragraph, area);
    }

    async fn handle_event(&mut self, _event: KeyEvent) -> bool {
        false
    }
}
#+end_src
    
    ちなみに、 ~handle_event()~ で ~bool~ を返しているのは、UIパーツ（Component）によってはキーマップが競合することがあるので、その場合親Componentにキーイベントを伝播しないようにするためのフラグです。

    描画のためのBackendもまんまBackendで定義しているおかげで、単体テストも簡単に書くことができます。

#+caption: Help構造体の単体テスト
#+begin_src rust
  fn test_case(help: &mut Help<TestBackend>, lines: Vec<&str>) {
      let mut terminal = get_test_terminal(20, 10);
      let lines = if !lines.is_empty() {
          lines
      } else {
          vec![
              "┌HELP──────────────┐",
              "│                  │",
              "│test message      │",
              "│12345             │",
              "│                  │",
              "│                  │",
              "│                  │",
              "│                  │",
              "│                  │",
              "└──────────────────┘",
          ]
      };
      let expected = Buffer::with_lines(lines);
      terminal
          .draw(|f| {
              help.draw(f, f.size());
          })
          .unwrap();
      terminal.backend().assert_buffer(&expected);
  }
#+end_src
    
    テスト用のBackendがtui-rsに付属してますので、それを使ってassertできます。

*** おわりに

    最近こっちに時間割けて無いのであれですが、ちょこちょこ直しておこうと思います。
    
    FBI！

** DONE Serverless,ECS（Fargate）自動デプロイ環境の構築 :AWS:Terraform:Serverless:CICD:Lambda:Laravel:
   :PROPERTIES:
   :EXPORT_FILE_NAME: auto-deploy-with-terraform-serverless
   :EXPORT_HUGO_SECTION: /posts/2021/05
   :EXPORT_DATE: 2021-05-24
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
   
   デプロイを自動化するのがMustになりつつありますが、なかなか完璧な自動デプロイ環境を作るのは難しいなーと感じています。
   
   で、最近、所属会社の経営層へのプレゼンのネタとして、中途半端にデプロイ自動化しているプロジェクトを、全リソース自動デプロイ化したら面白いんじゃね？という点で色々検証してみましたので、それについての記事になります。

<!--more-->

*** 課題
**** 現状
     現状のデプロイ構成はこんな感じでした。

#+begin_src python :exports none
from diagrams import Diagram, Cluster, Edge
from diagrams.aws.storage import S3
from diagrams.aws.compute import EC2, LambdaFunction
from diagrams.aws.general import User, Users, InternetGateway, InternetAlt1
from diagrams.onprem.client import Client
from diagrams.aws.network import ALB, VPC, PublicSubnet, PrivateSubnet, CF, NATGateway
from diagrams.aws.storage import S3
from diagrams.aws.database import RDSMysqlInstance
from diagrams.aws.devtools import Codepipeline, Codecommit, Codedeploy, Codebuild
from diagrams.aws.integration import SimpleNotificationServiceSnsEmailNotification
from diagrams.generic.blank import Blank
from diagrams.programming.flowchart import Document

node_attr = {
    "fontsize": "20",
    "fontname": "Helvetica-Bold"
}

with Diagram('', node_attr=node_attr, filename='images/2021/05/24/before-structure'):
  # End User
  user = User('\nEnd User')

  # Developers
  developers = Users('\nDevelopers')
  source = Document('\nSource Code')

  internet = InternetAlt1('\nwww')

  pipeline = Codepipeline('\nCodePipeline')
  codecommit = Codecommit('\nCodeCommit')
  codebuild = Codebuild('\nCodeBuild')
  codedeploy = Codedeploy('\nCodeDeploy')

  with Cluster('Development'):
    dev = [
      Blank('\nProductionと同じようなインフラ')
    ]
  
  with Cluster('Staging'):
    stg = [
      Blank('\nProductionと同じようなインフラ')
    ]

  with Cluster('Production'):
    sns = SimpleNotificationServiceSnsEmailNotification('\nSNS')

    with Cluster('VPC'):
      nat = NATGateway('\nNAT Gateway')
      igw = InternetGateway('\nInternet Gateway')

      with Cluster('Public Subnets'):
        alb = ALB('\nALB')

      with Cluster('Private Subnets'):
        with Cluster('RDS Cluster'):
          rds = RDSMysqlInstance('\nDB-master')
          rds - [RDSMysqlInstance('\nDB-ro')]

        private_subnets = [
          EC2('\nWeb 1'),
          EC2('\nWeb 2')
        ]

    with Cluster('Lambda Functions'):
      fns = LambdaFunction('\nLambda 1')
      fns - LambdaFunction('\nLambda 2..X')

    with Cluster('URL Shortener'):
      url_s3 = S3('\nShorten URL Info Bucket')
      shortener = [
        LambdaFunction('\n短縮化API\nリダイレクトAPI')
      ]

    s3 = S3('\nS3 Bucket')
    cf = CF('\nClloudFront')

    developers - source >> Edge(label="Push", fontsize="20", fontname="Helvetica-Bold") >> codecommit >> pipeline >> codebuild >> codedeploy >> private_subnets >> sns >> user
    codedeploy >> s3
    private_subnets >> rds
    private_subnets >> nat >> internet
    alb >> igw >> internet
    user >> Edge() << alb >> private_subnets
    user >> Edge() << cf >> s3
    user >> Edge() << shortener >> url_s3
    developers >> Edge() << dev << codedeploy
    developers >> Edge() << stg << codedeploy
#+end_src

#+caption: 現状のデプロイ構成
[[file:images/2021/05/24/before-structure.png]]

     単純化するとこんな感じ。

#+begin_src python :exports none
from diagrams import Diagram, Cluster, Edge
from diagrams.aws.compute import EC2, LambdaFunction
from diagrams.aws.general import Users
from diagrams.aws.network import ALB
from diagrams.aws.database import RDSMysqlInstance
from diagrams.aws.devtools import Codepipeline, Codecommit, Codedeploy, Codebuild
from diagrams.programming.flowchart import Document
from diagrams.aws.devtools import Codepipeline

node_attr = {
    "fontsize": "20",
    "fontname": "Helvetica-Bold"
}

with Diagram('', filename='images/2021/05/24/before-simple-structure', node_attr=node_attr):
  consumers = Users(label='\n利用者')
  developers = Users('\n開発者')
  web = EC2('\nWebサーバ')
  lb = ALB('\nロードバランサ')
  db = RDSMysqlInstance('\nデータベース')
  sls = LambdaFunction('\nLambda関数\n（API）')
  pipe = Codepipeline('\n自動デプロイ\nツール')

  app_src = Document('\nアプリソース')
  conf = Document('\nミドル設定ファイル')

  sls_src = Document('\nLambdaソース')

  consumers >> Edge() << lb >> web >> db
  web >> Edge() << sls
  web - Edge(style="dashed") - conf
  sls - Edge(style="dashed") - sls_src
  developers - Edge(style="bold") - app_src >> pipe >> Edge(label="自動デプロイ", style="bold", fontsize='20', fontname="Helvetica-Bold") >> web
#+end_src

#+caption: 現状のデプロイ構成（簡易版）
[[file:images/2021/05/24/before-simple-structure.png]]

     まとめると下記のような感じです。

     - アプリケーション（このプロジェクトではLaravelでした）のソースはデプロイ自動化できている
     - ミドルウェアの設定ファイルは手動デプロイ
     - サーバレス（Lambda）のソースは手動デプロイ
     - インフラの設定情報も当然手動でしか変更不可

**** 現状の運用の辛い点

     - デプロイするリソースによって手順が変わる（レビュア、デプロイ担当者の負担増）
     - 手動デプロイソースはGit管理できていない
     - インフラ設定が知らないうちに変わるリスク（現状の運用では発生していない）
     - オペミスリスク

     と、やっぱりある程度システムが複雑になってくると結構辛い点が出てきます。

*** 作ってみた
    ということで、後述のツールを使って実際に全部自動化したものがこちら。

    [[https://github.com/zeroclock/terraform-practice-01][github zeroclock/terraform-practice-01]]

    それぞれ、 ~infra~ がTerraformの定義ファイルで、 ~app~ 配下がWebアプリ（Laravel）のソースと、サーバレス（Lambda）のソースになっています。

**** 使うツール
    ソースとかインフラ系の設定ファイルを全てコード化するとどんなもんになるんだろうと思い、下記のツールを使って同じような環境を作ってみました。

    - [[https://www.terraform.io/][Terraform]]：パブリッククラウドのインフラ構成をコード化して自動構築してくれるツール
    - [[https://www.serverless.com/][Serverless framework]]：サーバレス系（Lambda,S3など）のインフラ構築とアプリソースのデプロイを自動化してくれるツール

    似たようなツールを使ってますが、棲み分けとしては下記のような感じです。

    - 複数のリソースから参照されるリソース：Terraform
    - 特定のリソースからのみ参照されるリソース：Serverless

#+caption: 両ツールの棲み分けについてのソース（https://www.serverless.com/blog/definitive-guide-terraform-serverless）
#+begin_quote
For application-specific infrastructure, we suggest managing all the pieces with the Serverless Framework, for a few reasons.

...

However, coupling shared infrastructure to a specific application isn’t correct. Shared infrastructure will usually get updated instead of re-created from scratch.
#+end_quote

    例えばWebサーバとLambda、それぞれに参照されるDBがあるとすると、WebサーバとDBは ~Terraform~ で定義し、Lambda関数とそれに関連するAPI Gatewayや実行ロール等は ~Serverlss~ で定義するような感じです。

**** 構成

    インフラ＆デプロイ構成は下記の通り。

#+begin_src python :exports none
from diagrams import Diagram, Cluster, Edge
from diagrams.aws.storage import S3
from diagrams.aws.compute import LambdaFunction, ElasticContainerServiceContainer, ECR, ECS
from diagrams.aws.general import User, Users, InternetGateway, InternetAlt1
from diagrams.onprem.client import Client
from diagrams.aws.network import ALB, VPC, PublicSubnet, PrivateSubnet, CF, Privatelink
from diagrams.aws.storage import S3
from diagrams.aws.database import RDSMysqlInstance
from diagrams.aws.devtools import Codepipeline, Codecommit, Codedeploy, Codebuild
from diagrams.generic.blank import Blank
from diagrams.programming.flowchart import Document
from diagrams.aws.integration import SQS
from diagrams.onprem.vcs import Github
from diagrams.onprem.ci import TravisCI
from diagrams.custom import Custom

node_attr = {
    "fontsize": "20",
    "fontname": "Helvetica-Bold"
}

with Diagram('', filename='images/2021/05/24/after-structure', node_attr=node_attr):
  # Custom Icons
  terraform = Custom("\nTerraform", "../../../terraform-logo.png")
  sls = Custom("\nServerless", "../../../sls-logo.png")

  # End User
  user = User('\nEnd User')

  # Developers
  developers = Users('\nDevelopers')
  source = Document('Source Code(Laravel)\nSource Code(Lambda)\nInfra configuration')

  internet = InternetAlt1('\nwww')

  github = Github('\nGithub')
  travis = TravisCI('\nTravisCI')

  travis - [terraform, sls]

  ecr = ECR('\nECR')
  sqs = SQS('\nSQS')

  with Cluster('\nDevelopment'):
    dev = Blank('\nProductionと\n同じようなインフラ')
  
  with Cluster('Staging'):
    stg = Blank('\nProductionと\n同じようなインフラ')

  with Cluster('\nProduction'):
    with Cluster('\nVPC'):
      igw = InternetGateway('\nInternet Gateway')

      with Cluster('\nPublic Subnets'):
        alb = ALB('\nALB')

      with Cluster('\nPrivate Subnets'):
        with Cluster('\nRDS Cluster'):
          rds = RDSMysqlInstance('\nDB')

        with Cluster('\nECS Cluster'):
          worker = ElasticContainerServiceContainer('\nQueue Worker')
          ecs = ECS('\nECS')
          # link = Privatelink('VPC Privatelink')
          private_subnets = [
            ElasticContainerServiceContainer('\nApache'),
            ElasticContainerServiceContainer('\nLaravel'),
            ElasticContainerServiceContainer('\nBatch'),
            worker
          ]
          ecs - private_subnets


    fns = LambdaFunction('\nLambda 1')

    s3 = S3('\nS3 Bucket')
    cf = CF('\nClloudFront')

    sls >> fns
    developers - source >> Edge(label="Push", fontsize="20", fontname="Helvetica-Bold") >> github >> travis
    fns >> sqs >> worker
    ecr >> ecs
    private_subnets >> rds
    alb >> igw >> internet
    user >> Edge() << alb >> private_subnets
    user >> Edge() << cf >> s3
    # developers >> Edge() << dev
    # developers >> Edge() << stg
    terraform >> [
      ecr,
      sqs,
      rds,
      igw,
      cf,
      alb,
      ecs,
      dev,
      stg
    ]
#+end_src

#+caption: 改善後のインフラ構成
[[file:images/2021/05/24/after-structure.png]]

#+begin_src python :exports none
from diagrams import Diagram, Cluster, Edge
from diagrams.aws.compute import EC2, LambdaFunction, ECS
from diagrams.aws.general import Users
from diagrams.aws.network import ALB
from diagrams.aws.database import RDSMysqlInstance
from diagrams.aws.devtools import Codepipeline, Codecommit, Codedeploy, Codebuild
from diagrams.programming.flowchart import Document
from diagrams.onprem.ci import TravisCI
from diagrams.custom import Custom

node_attr = {
    "fontsize": "20",
    "fontname": "Helvetica-Bold"
}

with Diagram('', filename='images/2021/05/24/after-simple-structure', node_attr=node_attr):
  consumers = Users('\n利用者')
  developers = Users('\n開発者')
  web = ECS('\nWebサーバ')
  lb = ALB('\nロードバランサ')
  db = RDSMysqlInstance('\nデータベース')
  sls = LambdaFunction('\nLambda関数\n（API）')
  travis = TravisCI('\n自動デプロイ\nツール')
  terraform = Custom('\nIaCツール', '../../../terraform-logo.png')

  app_src = Document('\nアプリソース\nミドル設定ファイル\nLambdaソース\nインフラ構成ファイル')
    # conf = Document('ミドル設定ファイル')
    # sls_src = Document('Lambdaソース')

  consumers >> Edge() << lb >> web >> db
  web >> Edge() << sls
  developers - Edge(style="bold", fontsize="20", fontname="Helvetica-Bold") - app_src - travis
  travis - terraform >> Edge(label='自動デプロイ', style="bold", fontsize="20", fontname="Helvetica-Bold") >> [web, sls]
#+end_src

#+caption: 改善後のインフラ構成（簡易版）
[[file:images/2021/05/24/after-simple-structure.png]]

    SQSとLambdaが連携してたり、Laravelでバッチ用のコンテナがあったりで色々それっぽく作りました。Laravelについては、パブリックリポジトリにしている関係でenv系を暗号化するライブラリを入れていたりしてますが構成はほぼプレーンな状態です。

**** 苦労したところ

     インフラ定義についてはそれぞれのファイルを見ていただくとして、大変だったのがTravisCIを使ったECSへのデプロイ周り（Terraform/sls関係ないじゃんというツッコミは置いといて...）。

     ~container definition~ とか ~task definition~ についてはデプロイ時ダイナミックに値を入れたりしないといけないので、 ~infra/code_deploy~ 配下のシェルスクリプトで若干力技で修正してapplyしています。

#+caption: 新しいtask_definitionを発行するシェルスクリプト（infra/code_deploy/get_new_taskdef.sh）
#+begin_src sh
#!/bin/bash

# set -eux

TAG=$1
APP_NAME=$2
REGION=$3

# get current taskdef
# ref: https://github.com/aws/aws-cli/issues/3064#issuecomment-784614089
TASK_DEFINITION=$(aws ecs describe-task-definition --task-definition ${APP_NAME} --region ${REGION} \
      --query '{  containerDefinitions: taskDefinition.containerDefinitions,
                  family: taskDefinition.family,
                  taskRoleArn: taskDefinition.taskRoleArn,
                  executionRoleArn: taskDefinition.executionRoleArn,
                  networkMode: taskDefinition.networkMode,
                  volumes: taskDefinition.volumes,
                  placementConstraints: taskDefinition.placementConstraints,
                  requiresCompatibilities: taskDefinition.requiresCompatibilities,
                  cpu: taskDefinition.cpu,
                  memory: taskDefinition.memory}')
ORIGINAL_TEXT=$(echo $TASK_DEFINITION | jq '.containerDefinitions[] | .image' -r | uniq)
REPLACED_TEXT=$(echo ${ORIGINAL_TEXT} | sed -z "s/:\S*/:$TAG/g")
ORIGINAL_ARR=($(echo $ORIGINAL_TEXT))
REPLACED_ARR=($(echo $REPLACED_TEXT))

for i in $(seq 1 ${#ORIGINAL_ARR[@]})
do
  # escape
  ORIGIN=$(echo ${ORIGINAL_ARR[i-1]} | sed 's/\./\\\./g' | sed 's/\//\\\//g')
  REPLACED=$(echo ${REPLACED_ARR[i-1]} | sed 's/\./\\\./g' | sed 's/\//\\\//g')
  TASK_DEFINITION=$(echo $TASK_DEFINITION | sed -z s/$ORIGIN/$REPLACED/g)
done

echo $TASK_DEFINITION >&1
#+end_src

     ここがクリアできさえすれば、あとはポチポチブラウザでインフラ構築するような感覚で定義ファイル修正＆ ~terraform apply~ すれば良いので、わりと楽でした。

*** 考察
**** 良さそうな点

     こうしてインフラ構成やミドル構成などまるっとデプロイ自動化することで、先程挙げた点が解消できそうだなと。

     - デプロイするリソースによって手順が変わる（レビュア、デプロイ担当者の負担増）

       →全てGit管理されているので、同じ手順（コードレビュー、デプロイ実行）でデプロイできる

     - 手動デプロイソースはGit管理できていない

       →Git管理できている

     - インフラ設定が知らないうちに変わるリスク（現状の運用では発生していない）

       →変更契機がGitへのPush時のみに限定されるので、変更が追いやすい

     - オペミスリスク

       →自動デプロイなのでリスク軽減

     他にも、きちんと作ればインフラのマイグレが簡単にできたり、色々良いことずくめな気がしています。

**** 微妙なところ（リスク）

     ただし、リスクもあるかと思います。

     - 学習コスト

       →自分はもともとserverless使ってたので1,2md程度でチュートリアルやるだけで一応動くものは作れましたが、馴染みがない人だと若干苦労しそう。

     - 保守できる人が限定される可能性

       →学習コストにも関連しますが、Terraformやslsに詳しい人が社内で増えないとメンテナンスできる人が限定されてしまうリスクもありそう（これはモダン言語で作ったりすると起こるような問題に近い）。

     - デプロイ設計きちんとできるプロジェクトじゃないと難しい

       →普段からCodePipelineとかのサービス使って、それなりにデプロイフローを構築・運用できるチームじゃないと難しい面はあります。

*** さいごに
    最近は ~Code as Infrastructure~ 的な感じで色々なリソースをコード化しようっていう動きが活発化しているので、普段やってる案件でも取り込めたら良いですね。

    実際、インフラ担当になった案件ではLambdaを一部Serverlessによるデプロイに切り替えたりしていますが、わりといい感じな気がしています。

    ただ、デプロイ自動化までできて50点だと思っており、そこからさらにテスト自動化までできてやっと及第点な感じがするので、まだまだ先は長い...
** TODO 【OSS貢献記録】Fuchsia/net stack3 issue 48969
   :PROPERTIES:
   :EXPORT_FILE_NAME: oss-contribute-fuchsia-issue-48969
   :EXPORT_HUGO_SECTION: /posts/2021/05
   :EXPORT_DATE: 2021-05-24
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

   https://bugs.fuchsia.dev/p/fuchsia/issues/detail?id=48969

   該当箇所はここっぽい。

   https://cs.opensource.google/fuchsia/fuchsia/+/main:src/connectivity/network/netstack3/src/bindings/socket/mod.rs;l=145-156

   ソースを見た感じ、posixのsocket関連のリクエストを捌く処理。

   今は下記の通り、スタブアウトされており、未対応という形になっている。

#+begin_src rus
// TODO(https://fxbug.dev/48969): implement this method.
responder_send!(responder, &mut Err(zx::Status::NOT_FOUND.into_raw()));
#+end_src

   で、この ~handle_fidl_socket_provider()~ 自体はWorkerのイベントループで使用される。

   さて、ということで、こやつらは一体どんなI/Oを持つべきなのかということで、FIDLの定義ファイルを探す。
   （FIDLというのはFuchsiaにおけるプロセス間通信に用いられるインタフェース定義言語。中間言語のような位置づけなので、実装言語が異なっていても互いにデータ通信することができる）

   ソースがでかすぎるのでPJ直下でgrep

#+begin_src
$ rg -F "InterfaceIndexToName"
sdk/fidl/fuchsia.posix.socket/socket.fidl
579:    InterfaceIndexToName(uint64 index) -> (interface_name name) error zx.status;

sdk/lib/fdio/socket.cc
1280:      auto response = provider->InterfaceIndexToName(static_cast<uint64_t>(ifr->ifr_ifindex));

src/connectivity/network/netstack3/src/bindings/socket/mod.rs
146:            psocket::ProviderRequest::InterfaceIndexToName { index: _, responder } => {

src/connectivity/network/netstack/fuchsia_posix_socket.go
2468:func (sp *providerImpl) InterfaceIndexToName(_ fidl.Context, index uint64) (socket.ProviderInterfaceIndexToNameResult, error) {
2470:           return socket.ProviderInterfaceIndexToNameResultWithResponse(socket.ProviderInterfaceIndexToNameResponse{
2474:   return socket.ProviderInterfaceIndexToNameResultWithErr(int32(zx.ErrNotFound)), nil

#+end_src

   お、netstack（Go言語の方）の実装も見つかったので後で参考にしよう。

   該当するFIDLの定義箇所はこちら。

   https://cs.opensource.google/fuchsia/fuchsia/+/main:sdk/fidl/fuchsia.posix.socket/socket.fidl;l=577-585

#+begin_src
    /// Looks up an interface by its index and returns its name. Returns `ZX_ERR_NOT_FOUND` if the
    /// specified index doesn't exist.
    InterfaceIndexToName(uint64 index) -> (interface_name name) error zx.status;
    /// Looks up an interface by its name and returns its index. Returns `ZX_ERR_NOT_FOUND` if the
    /// specified name doesn't exist.
    InterfaceNameToIndex(interface_name name) -> (uint64 index) error zx.status;
    /// Looks up an interface by its name and returns its flags. Returns `ZX_ERR_NOT_FOUND` if the
    /// specified name doesn't exist.
    InterfaceNameToFlags(interface_name name) -> (InterfaceFlags flags) error zx.status;
#+end_src

   ~interface_name~ は、同ファイルに定義されてますが、最大15byteの文字列のようです。

#+begin_src 
/// The maximum length of an interface name.
// `sizeof((struct ifreq).ifr_name) == 16`; the last byte is reserved for the null terminator.
const uint8 INTERFACE_NAME_LENGTH = 15;

/// An interface name as a sequence of bytes.
alias interface_name = string:INTERFACE_NAME_LENGTH;
#+end_src

    次に、GOの実装を見てみる。

    https://cs.opensource.google/fuchsia/fuchsia/+/main:src/connectivity/network/netstack/fuchsia_posix_socket.go;l=2468-2486

#+begin_src go
func (sp *providerImpl) InterfaceIndexToName(_ fidl.Context, index uint64) (socket.ProviderInterfaceIndexToNameResult, error) {
	if info, ok := sp.ns.stack.NICInfo()[tcpip.NICID(index)]; ok {
		return socket.ProviderInterfaceIndexToNameResultWithResponse(socket.ProviderInterfaceIndexToNameResponse{
			Name: info.Name,
		}), nil
	}
	return socket.ProviderInterfaceIndexToNameResultWithErr(int32(zx.ErrNotFound)), nil
}
#+end_src

    ~providerImpl~ 構造体が ~Netstack~ 、 ~Stack~ と辿って ~NICInfo()~ でNIC情報取得し、該当するindexの情報があればそれを返却するような流れ。

    Netstack: https://cs.opensource.google/fuchsia/fuchsia/+/main:src/connectivity/network/netstack/netstack.go;l=180

    Stack: https://cs.opensource.google/fuchsia/fuchsia/+/main:third_party/golibs/vendor/gvisor.dev/gvisor/pkg/tcpip/stack/stack.go;l=70

    Go製のNetstackはgvisorに依存した実装になっているけど、Netstack3は（Stack関連については）特にthirdparyに依存しているわけでも無さそうなので、FIDLを確認する。

    なんとなく ~interface_id~ とか ~interface_name~ とかでgrepかけたらそれっぽいところを発見。

    https://cs.opensource.google/fuchsia/fuchsia/+/main:sdk/fidl/fuchsia.net.stack/stack.fidl;l=35-78

    それっぽいのは ~InterfaceProperties~ なので、再びgrep。

    https://cs.opensource.google/fuchsia/fuchsia/+/main:src/connectivity/network/netstack3/src/bindings/stack_fidl_worker.rs

#+begin_src rust
    fn fidl_list_interfaces(self) -> Vec<fidl_net_stack::InterfaceInfo> {
        let mut devices = Vec::new();
        for device in self.ctx.dispatcher().get_inner::<Devices>().iter_devices() {
            let mut addresses = Vec::new();
            if let Some(core_id) = device.core_id() {
                for addr in get_all_ip_addr_subnets(&self.ctx, core_id) {
                    match addr.try_into_fidl() {
                        Ok(addr) => addresses.push(addr),
                        Err(e) => {
                            error!("failed to map interface address/subnet into FIDL: {:?}", e)
                        }
                    }
                }
            };
            devices.push(InterfaceInfo {
                id: device.id(),
                properties: InterfaceProperties {
                    name: "[TBD]".to_owned(), // TODO(porce): Follow up to populate the name
                    topopath: device.path().clone(),
                    filepath: "[TBD]".to_owned(), // TODO(porce): Follow up to populate
                    mac: Some(Box::new(device.mac())),
                    mtu: device.mtu(),
                    features: device.features(),
                    administrative_status: if device.admin_enabled() {
                        AdministrativeStatus::Enabled
                    } else {
                        AdministrativeStatus::Disabled
                    },
                    physical_status: if device.phy_up() {
                        PhysicalStatus::Up
                    } else {
                        PhysicalStatus::Down
                    },
                    addresses, // TODO(gongt) Handle tentative IPv6 addresses
                },
            });
        }
        devices
    }
#+end_src

    で、ここで（今更）Netstack3のサービス構成を今更確認すると、下記のように、3つのサービスを作成していることがわかる。

    https://cs.opensource.google/fuchsia/fuchsia/+/main:src/connectivity/network/netstack3/src/bindings/mod.rs;l=474-494

#+begin_src rust
    /// Consumes the netstack and starts serving all the FIDL services it
    /// implements to the outgoing service directory.
    pub async fn serve(self) -> Result<(), anyhow::Error> {
        debug!("Serving netstack");
        self.spawn_timers().await;
        let mut fs = ServiceFs::new_local();
        fs.dir("svc")
            .add_fidl_service(|rs: fidl_fuchsia_net_icmp::ProviderRequestStream| {
                icmp::IcmpProviderWorker::spawn(self.clone(), rs)
            })
            .add_fidl_service(|rs: fidl_fuchsia_net_stack::StackRequestStream| {
                stack_fidl_worker::StackFidlWorker::spawn(self.clone(), rs)
            })
            .add_fidl_service(|rs: fidl_fuchsia_posix_socket::ProviderRequestStream| {
                socket::SocketProviderWorker::spawn(self.clone(), rs)
            });
        fs.take_and_serve_directory_handle()?;
        fs.collect::<()>().await;
        debug!("Services stream finished");
        Ok(())
    }
#+end_src

    1. fidl/fuchsia.net.icmp
    2. fidl/fuchsia.net.stack
    3. fidl/fuchsia.posix.socket

    各サービスにおいては、 ~ctx: Arc<Mutex<Context<BindingsDispatcher>>>~ が共有されてますので、先述のInterfacePropertiesを ~ctx~ から取得する処理を使えそうです。

    ただし、現状は ~stack_fidl_worker~ で定義されちゃってますので、共通化する必要がありそうです。
** DONE インターン生の研修環境をTerraformとcloud9でサクッと作った話 :Terraform:AWS:cloud9:
   :PROPERTIES:
   :EXPORT_FILE_NAME: intern-cloud9-env-with-terraform
   :EXPORT_HUGO_SECTION: /posts/2021/06
   :EXPORT_DATE: 2021-06-03
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

   所属会社にて、内定者向けのインターン（っていうのか？）を開催することになり、急遽環境構築をすることになりました。

   内容も結構本格的で、ほぼ実案件のソースを使ってバグ改修とか機能追加とかさせたいね　ということでした。まあソースとかは一応持ち帰りで作ってる案件のソースもあるし、バグについても過去のバグチケットを漁れば良いな　と。

   あとはただ一つ、インターン担当者の思いは...

<!--more-->

   *出社したくねえ...＼(^o^)／*

   ということで、準備ももちろん、実施もオンラインでも対応できるような形を目指して環境構築がスタートしましたとさ。


*** 作った環境
**** 要件
     - ブラウザ経由でソースコードの編集、及び動作確認ができること
     - *セキュアであること*
     - 環境構築が簡単なこと

**** ソースコード

     Terraformで作ってGithubに置いておきました。

     [[https://github.com/zeroclock/cloud9-training-env-terraform][Github/zeroclock - cloud9-training-env-terraform]]

**** 構成図

     ざっくりとこんな感じの環境を作りました。

#+begin_src plantuml :file overview.svg :cmdline -config "$HOME/.emacs.d/styles.uml" :async
!include <awslib/AWSCommon>
!include <awslib/AWSSimplified>

!include <awslib/General/User>
!include <awslib/Compute/EC2>
!include <awslib/DeveloperTools/Cloud9>
!include <awslib/NetworkingAndContentDelivery/VPCNATGateway>
!include <awslib/ManagementAndGovernance/SystemsManager>

left to right direction

SystemsManager(smgr, "SystemsManager", "")
User(intern_mem, "internship participant", "")
User(support_mem, "supporting member", "")

cloud "www" as www {
  [Git]
  [Library]
}

cloud "VPC" as vpc {
  node "Public Subnet" as pub_sub {
    VPCNATGateway(nat, "NAT GW", "")
  }
  node "Private Subnet" as pri_sub {
    Cloud9(ide, "Cloud9[EC2]", "")
  }
}

intern_mem -up-> smgr
smgr -> ide
support_mem -up-> smgr
www <--> nat
nat <-- ide
#+end_src

#+caption: インフラ構成概要図
#+RESULTS[507f30e08fd746160519a2e6bcab99c4c9da40ff]:
[[file:overview.svg]]

**** 構成について
     
     クラウド上でIDEを使うとなると、個人的にはCloud9一択でした。とはいえ、数年前に勉強のためにちょろっと触ったくらいだったので、何ができるかについては若干手探りで。

     最近Cloud9がSystemsManager経由で利用可能になった（≒プライベートサブネットで利用可能になった）という話を聞いており、パブリックにインスタンスを置かなくても良さそうだったという部分も後押しになった形。

     > 注意ですが、[[https://dev.classmethod.jp/articles/cloud9-ide-network-condition/][この記事]]にあるように、AWS側としてはプライベートサブネットではなく「インバウンド通信を遮断した（no-ingressな）パブリックサブネット」での利用を意図しているようですね。ただ、今回は特にaws cliを使ったりしないためプライベートサブネット上に構築しました。

     また、できるだけ簡単に環境構築ができるように、「作成するIAMユーザの数」と「作成する環境の数」を変数化して、動的に変更できるようにしました。

     というのも、まだ具体的な参加人数が決まっていないため、実際の環境構築時にダイナミックに変更可能にしたかったためです。

     ソースコードだとこの辺です。

     https://github.com/zeroclock/cloud9-training-env-terraform/blob/master/user/main.tf

#+caption: TerraformによるIAMユーザの動的な変更
#+begin_src terraform
resource "aws_iam_user" "this" {
    count = var.user_count
    name = "${var.name}-${count.index}"
    path = "/"
    force_destroy = true
}
#+end_src

    また、今回はCloud9のインスタンス内でDockerコンテナを起動して、それをlocalhostで見に行く感じになりますので、クラウド上でローカル開発環境を作るようなイメージです。

    （いくつかハマりポイントがありますので後述します）

*** 構築フロー
    今回はTerraformを使用したので、かなり簡単に環境の作成＆削除が行えます。

    具体的な構築手順は下記の通り。

#+begin_src plantuml :file overview2.svg :cmdline -config "$HOME/.emacs.d/styles.uml" :async
start

:IAMアカウント情報、ARNを取得;
:GithubからソースをPull;
:terraform init;
:terraform plan;
:terraform apply;
:各環境にて環境整備;

stop
#+end_src

#+caption: 環境構築手順
#+RESULTS:
[[file:overview2.svg]]

    インフラの手動構築（AWSのコンソールでのポチポチ）は不要ですが、各環境内部で実施する作業は、 +自動化する時間がありませんでした+ 自動化しませんでした。

    - Gitからの研修用ソースのPull：これはCodeCommitのみ対応なので、今回はどちらにしても無理だった
    - dockerイメージのビルド：ECRにイメージPushでそれをPullでいいかも
    - 依存パッケージのインストール
    - env系の修正

*** ハマりポイント
**** プレビューが表示されない
     プレビューでアクセスする際のポートは8080にしないと死にます（噂によると8081,8082とかも大丈夫らしいけど8080にしといた方が無難）

**** セッションが死ぬ
     デバッグログで見るとわかりますが、envとかの設定でcookie発行時のドメイン情報がlocalhostのままになってたりすると、3rd-party cookieで弾かれます。ログインできなくて若干ハマりました。

**** localhostじゃないのでドメイン設定に気をつける
     プレビューを表示すると、ドメインがhogehoge.vfs~みたいな構成になってますので、そのドメイン名をアプリ側にも設定しておきます（Cookieの件と似てますね）

*** さいごに

    そんな感じで作ってみた環境ですが、t2.smallとかのインスタンスでもわりと快適に動いてますので、意外と使い所多いかも？と思いました。

    ただ、一点注意なんですが、プライベートサブネットなのでインターネット通信のためにNAT GWが必要です。

    NAT GWは利用料金が跳ね上がる原因の上位にランクインするやばいやつなので、ご利用は計画的に・・・。
    （多分一ヶ月起動しっぱとかだとインスタンス諸々含め2万くらいとられます...）

** DONE Typescriptを使用したサーバレスWebsocketチャットサーバーの構築 :Typescript:Lambda:AWS:Websocket:React:
   :PROPERTIES:
   :EXPORT_FILE_NAME: websocket-chat-server-with-serverless
   :EXPORT_HUGO_SECTION: /posts/2021/06
   :EXPORT_DATE: 2021-06-06
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

   チャットサーバーの見積もり相談で、Websocket使った場合の実装を整理したので、メモっておきます。

   <!--more-->

*** 動作確認
    以下のURLで動作確認できます。

    https://reactplayground.zeroclock.dev/WebsocketChat

    - 複数タブで開く
      [[./images/2021/06/2021-06-06_21-18.png]]

    - Sign Upでユーザ登録＆ログイン
      [[./images/2021/06/2021-06-06_21-20.png]]

    - お互いのIDを教え合う
      [[./images/2021/06/2021-06-06_21-23.png]]

    - チャットする
      [[./images/2021/06/2021-06-06_21-24.png]]

    - 片方が切断すれば、きちんとステータスも変わります
      [[./images/2021/06/2021-06-06_21-25.png]]

    上記以外にも、切断後に再度ログインして相手IDを入力して再接続すると、過去のメッセージもきちんと再現してくれます（メッセージ情報の永続化）。

*** 構成図
    今回は、サーバレス（Lambda）でチャットサーバーを構築することを検討してみました。

#+begin_src plantuml :file overview.svg :cmdline -config "$HOME/.emacs.d/styles.uml" :async
!include <awslib/AWSCommon>
!include <awslib/AWSSimplified>

!include <awslib/General/User>
!include <awslib/Compute/Lambda>
!include <awslib/Database/DynamoDB>
!include <awslib/SecurityIdentityAndCompliance/Cognito>
!include <awslib/NetworkingAndContentDelivery/APIGateway2>

left to right direction

User(client, "Client(React)", "")
Cognito(cognito, "Cognito User Pool", "")
DynamoDB(db, "DynamoDB", "")
APIGateway2(apigw, "API GW", "")
Lambda(auth, "Auth", "")
Lambda(refresh, "Refresh Token", "")
Lambda(validate, "Authorizer", "")

node "Websocket handlers" as handlers {
  Lambda(conn, "handleSocketConnect", "")
  Lambda(send, "sendMsg", "")
  Lambda(get, "getMsg", "")
  Lambda(status, "status", "")
  Lambda(disconn, "handleSocketDisconnect", "")
}

client <--> apigw

apigw <--> auth
auth --> cognito

apigw <--> refresh
refresh --> cognito

apigw --> validate

validate --> conn
conn <--> db

validate --> send
send <--> db

validate --> get
get <-- db

validate --> status
status <-- db

validate --> disconn
disconn <--> db

apigw <-- handlers
#+end_src

#+caption: 概要図
#+RESULTS:
[[file:overview.svg]]

    API GatewayがWebsocketのフロントエンドとして機能し、リクエストに応じて各バックエンド(handlers)にリクエストを流します。

*** 処理フロー

    ちょっと図だとわかりにくいので、Websocketに絞った処理の流れを下記に示します。

#+caption: 処理フロー
#+begin_quote
    - [Client -> Server] Websocket通信確立リクエスト送信（with アクセストークン）
    - [Server] ~Authorizer~ においてトークンの検証処理実行
    - [Client <- Server] 200 OK
    - [Client -> Server] ~$connect~ request
    - [Server] ~handleSocketConnect~ において、DynamoDBに接続情報を登録し、通信相手がすでにONLINEの場合は、ステータス更新情報を *WebSocket経由で* 送信
    - [Client <- Server] 200 OK
    - [Client -> Server] ~GETMSG~ request
    - [Server] ~getMsg~ において、DynamoDBから該当するconnectionに紐づくメッセージ情報を取得し、 *WebSocket経由で* 送信
    - [Client <- Server] 200 OK
    - ...
#+end_quote

    Websocketはイベント駆動なので、 ~GETMSG~ のリクエストを送信しても、 *そのレスポンスとしてメッセージ情報が返却されるわけではない* ことに注意が必要です。

    返却情報については、ClientからServerに送信したのと同じように、ServerからClientに対して ~GETMSG~ イベント（異なるイベントの可能性も有り）としてリクエストが飛んでくるので、それをClient側でハンドリングすることで取得します。

    よって、例としてメッセージ配信後に相手が接続を切った場合の処理フローは下記のようになります。

#+begin_src plantuml :file overview2.svg :cmdline -config "$HOME/.emacs.d/styles.uml" :async
actor Alice as alice
participant Server as server
actor Bob as bob

alice -> server: SENDMSG event
server -> server: Store message to DynamoDB
server -> bob: SENDMSG event
server <- bob: $disconnect event
server -> server: Update connection info in DynamoDB
server -> bob: STATUS event
alice <- server: STATUS event
#+end_src

#+caption: WebsocketによるP2P通信フロー
#+RESULTS:
[[file:overview2.svg]]

*** 実装

    ソースは下記にあります。

    - バックエンド：https://github.com/zeroclock/websocket-chat-server
    - フロントエンド：https://github.com/zeroclock/react-playground/tree/master/src/components/websocket_chat

    バックエンドとフロントエンドのうち、メッセージ送信に関連する処理をピックアップして解説します（AuthorizerやCognito連携周りの実装については省略）。

    なお、今回はFE、BEどちらもTypescriptで実装しました。

**** （Alice）フロントエンド（SENDMSGイベントの発火処理）
     https://github.com/zeroclock/react-playground/blob/master/src/components/websocket_chat/Chat.tsx

#+caption: フロントエンドのSENDMSGイベントの発火処理の実装
#+begin_src typescript
  const onSendMsg = () => {
    if (connection) {
      const data = {
        action: 'SENDMSG',
        message,
        timestamp: new Date().getTime(),
        fromSub: props.loginInfo.id,
        toSub: partnerId,
      }
      connection.send(JSON.stringify(data))
      const msg: Message = {
        message: data.message,
        fromSub: data.fromSub,
        toSub: data.toSub,
        timestamp: new Date(data.timestamp),
      }
      addMsg(msg)
    }
  }
#+end_src

    まず、メッセージを送信するにはフロント起点でイベントを送信しないといけません。

    とはいえ、やることとしては、予め確立しておいたWebsocket接続（ ~connection~ ）の ~send()~ を呼び出すだけです。

**** バックエンド（sendMessage）
     https://github.com/zeroclock/websocket-chat-server/blob/master/src/controllers/sendmsg.controller.ts

#+caption: バックエンドのSENDMSGイベントの処理
#+begin_src typescript
export const sendmsg: APIGatewayProxyHandler = async (event, _context) => {
  try {
    //const connectionId = event.requestContext.connectionId;
    const data = JSON.parse(event.body);

    const message = data['message'];
    const timestamp = data['timestamp'];
    const fromSub = data['fromSub'];
    const toSub = data['toSub'];

    const messageId = await dynamodbconnector.registerMessage(message, fromSub, toSub, timestamp);
    // send Message to partner user
    const sockets = await dynamodbconnector.findSocketsBySub(toSub);
    if (sockets.Count > 0) {
      const sendMessage: SocketMessage = {
        action: 'SENDMSG',
        message,
        fromSub,
        toSub,
        timestamp,
      };
      await apigatewayconnector.generateSocketMessage(
        sockets.Items[0].connectionId,
        JSON.stringify(sendMessage),
      );
    }

    return {
      statusCode: 200,
      headers: {
        'Content-Type': 'text/plain',
        'Access-Control-Allow-Origin': CONSTANTS.CORS_ORIGIN,
      },
      body: 'Greeting delivered',
    };
  } catch (e) {
    console.error('Failed to store message and send message to partner user', e);
    return {
      statusCode: 500,
      headers: {
        'Content-Type': 'text/plain',
        'Access-Control-Allow-Origin': CONSTANTS.CORS_ORIGIN,
      },
      body: 'Failed to store message and send message to partner user',
    };
  }
}
#+end_src

    フロント側からSENDMSGイベントが送信されましたので、バックエンドではsendMsgハンドラが呼び出されます。

    なお、途中で出てくる ~hogeconnector~ については、aws-sdkをラップした関数です。

    まず、取得したイベントデータから下記の情報を取り出し、DynamoDBにメッセージ情報を登録します。

    - message: メッセージ本文
    - timestamp: 送信日時のタイムスタンプ
    - fromSub: 送信元ID
    - toSub: 送信先ID

    続いて、送信先IDを元に送信先の ~connectionID~ を取得（ ~dynamodbconnector.findSocketsBySub()~ ）し、SENDMSGイベントを送信します（ ~apigatewayconnector.generateSocketMessage()~ ）。
     
**** （Bob）フロントエンド（SENDMSGイベントのハンドリング）
     https://github.com/zeroclock/react-playground/blob/master/src/components/websocket_chat/Chat.tsx

#+caption: フロントエンドのSENDMSGイベントのハンドリング処理の実装
#+begin_src typescript
  const initializeConnection = (c: WebSocket) => {
    c.onopen = WSOnOpen
    c.onclose = WSOnClose
    c.onerror = WSOnError
    c.onmessage = WSOnMessage
    setConnection(c)
  }
  // ...
  const WSOnMessage = (event: MessageEvent<any>) => {
    console.log('onMessage')
    // handle incoming message
    const data = JSON.parse(event.data)
    console.log(JSON.stringify(data))
    switch (data['action']) {
      case 'ISONLINE':
        // ...
      case 'SENDMSG':
        console.log('received message')
        console.log(`timestamp: ${data['timestamp']}`)
        const msg: Message = {
          message: data['message'],
          fromSub: data['fromSub'],
          toSub: data['toSub'],
          timestamp: new Date(data['timestamp']),
        }
        addMsg(msg)
        break
        // ...
        break
    }
  }
#+end_src

    Server側から送信されたSENDMSGイベントをハンドリングするには、 ~MessageEvent~ を引数とする関数を定義し、Websocketインスタンスの ~onmessage~ に登録します。

    actionによって処理を分岐させることで様々な種類のイベントをハンドリングすることが可能になります。

    SENDMSGイベントについては、取得した情報（メッセージ本文、timestamp等）を単にstateに保存するだけのシンプルな処理になります。

*** インフラの構築
    インフラについてはServerless frameworkを使用してデプロイします。

    https://github.com/zeroclock/websocket-chat-server/blob/master/serverless.ts

    詳細は上記ソースファイルをご参照ください。

    やってることとしては、API GWの設定、DynamoDBのテーブル作成、Lambda関数の実行ロール作成、CognioUserプールの作成等です。

*** さいごに
    API GatewayでWebsocketが対応したおかげで、かなり低コストでP2P通信が実現できることがわかって良かったです。

    サーバレスは従量課金なので、個人学習の強い味方。（EC2をプライベートサブネットに立ててNATGWのコストにビビる人は多いハズ）

    チャットとか、バックエンドの状況をできるだけリアルタイムにフロントに反映したい場合には重宝する技術ではありますが、注意も必要です。

    タブ開きっぱとかでコネクション張りっぱなしだとバックエンドの負荷が高まるため、一定時間操作が無かったら切断して、画面に戻ってきたら再接続するとか、しっかり使うのには色々やらないといけないことや考えないといけないことが多い技術かと思います。

    ajar（一定時間ごとにリクエスト投げて新規メッセージ等を確認する）なども検討すべきですね。
